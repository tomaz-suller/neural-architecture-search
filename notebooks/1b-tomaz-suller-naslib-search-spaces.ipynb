{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from naslib.utils import get_dataset_api\n",
    "from naslib import search_spaces\n",
    "from naslib.search_spaces.core import Metric\n",
    "import nasbench301\n",
    "\n",
    "from nas import _REPO_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAS-Bench-201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Could not find /home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/test-venv/lib/python3.12/site-packages/naslib/data/nb201_cifar10_full_training.pickle. Please download nb201_cifar10_full_training.pickle from https://drive.google.com/drive/folders/1rwmkqyij3I24zn5GSO6fGv2mzdEfPIEa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nasbench201_api \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnasbench201\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m nasbench201_api\n",
      "File \u001b[0;32m~/git/Politecnico/Extra/AI-Tech-Lab/nas/test-venv/lib/python3.12/site-packages/naslib/utils/get_dataset_api.py:213\u001b[0m, in \u001b[0;36mget_dataset_api\u001b[0;34m(search_space, dataset)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_nasbench101_api(dataset\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m search_space \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnasbench201\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_nasbench201_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m search_space \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnasbench301\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_nasbench301_api(dataset\u001b[38;5;241m=\u001b[39mdataset)\n",
      "File \u001b[0;32m~/git/Politecnico/Extra/AI-Tech-Lab/nas/test-venv/lib/python3.12/site-packages/naslib/utils/get_dataset_api.py:81\u001b[0m, in \u001b[0;36mget_nasbench201_api\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     73\u001b[0m     datafiles \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb201_cifar10_full_training.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcifar100\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb201_cifar100_full_training.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageNet16-120\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb201_ImageNet16_full_training.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mninapro\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb201_ninapro_full_training.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     }\n\u001b[1;32m     80\u001b[0m     datafile_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(get_project_root(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, datafiles[dataset])\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m     82\u001b[0m         datafile_path\n\u001b[1;32m     83\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatafile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatafiles[dataset]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124mhttps://drive.google.com/drive/folders/1rwmkqyij3I24zn5GSO6fGv2mzdEfPIEa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(datafile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     87\u001b[0m         data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Could not find /home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/test-venv/lib/python3.12/site-packages/naslib/data/nb201_cifar10_full_training.pickle. Please download nb201_cifar10_full_training.pickle from https://drive.google.com/drive/folders/1rwmkqyij3I24zn5GSO6fGv2mzdEfPIEa"
     ]
    }
   ],
   "source": [
    "nasbench201_api = get_dataset_api(\"nasbench201\", \"cifar10\")\n",
    "nasbench201_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = search_spaces.NasBench201SearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on NasBench201SearchSpace in module naslib.search_spaces.nasbench201.graph object:\n",
      "\n",
      "class NasBench201SearchSpace(naslib.search_spaces.core.graph.Graph)\n",
      " |  NasBench201SearchSpace(n_classes=10, in_channels=3)\n",
      " |\n",
      " |  Implementation of the nasbench 201 search space.\n",
      " |  It also has an interface to the tabular benchmark of nasbench 201.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      NasBench201SearchSpace\n",
      " |      naslib.search_spaces.core.graph.Graph\n",
      " |      torch.nn.modules.module.Module\n",
      " |      networkx.classes.digraph.DiGraph\n",
      " |      networkx.classes.graph.Graph\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, n_classes=10, in_channels=3)\n",
      " |      Initialise a graph. The edges are automatically filled with an EdgeData object\n",
      " |      which defines the default operation as Identity. The default combination operation\n",
      " |      is set as sum.\n",
      " |\n",
      " |      Note:\n",
      " |          When inheriting form `Graph` note that `__init__()` cannot take any parameters.\n",
      " |          This is due to the way how networkx is implemented, i.e. graphs are reconstructed\n",
      " |          internally and no parameters for init are considered.\n",
      " |\n",
      " |          Our recommended solution is to create static attributes before initialization and\n",
      " |          then load them dynamically in `__init__()`.\n",
      " |\n",
      " |          >>> def __init__(self):\n",
      " |          >>>     num_classes = self.NUM_CLASSES\n",
      " |          >>> MyGraph.NUM_CLASSES = 42\n",
      " |          >>> my_graph_42_classes = MyGraph()\n",
      " |\n",
      " |  encode(self, encoding_type=<EncodingType.ADJACENCY_ONE_HOT: 'adjacency_one_hot'>)\n",
      " |\n",
      " |  forward_before_global_avg_pool(self, x: torch.Tensor) -> list\n",
      " |\n",
      " |  get_arch_iterator(self, dataset_api=None) -> Iterator\n",
      " |\n",
      " |  get_hash(self) -> tuple\n",
      " |\n",
      " |  get_loss_fn(self) -> Callable\n",
      " |\n",
      " |  get_nbhd(self, dataset_api: dict = None) -> list\n",
      " |\n",
      " |  get_op_indices(self) -> list\n",
      " |\n",
      " |  get_type(self) -> str\n",
      " |\n",
      " |  mutate(self, parent: naslib.search_spaces.core.graph.Graph, dataset_api: dict = None) -> None\n",
      " |      This will mutate one op from the parent op indices, and then\n",
      " |      update the naslib object and op_indices\n",
      " |\n",
      " |  query(self, metric: naslib.search_spaces.core.query_metrics.Metric, dataset: str, path: str = None, epoch: int = -1, full_lc: bool = False, dataset_api: dict = None) -> float\n",
      " |      Query results from nasbench 201\n",
      " |\n",
      " |  sample_random_architecture(self, dataset_api: dict = None, load_labeled: bool = False) -> None\n",
      " |      This will sample a random architecture and update the edges in the\n",
      " |      naslib object accordingly.\n",
      " |\n",
      " |  sample_random_labeled_architecture(self) -> None\n",
      " |\n",
      " |  set_op_indices(self, op_indices: list) -> None\n",
      " |\n",
      " |  set_spec(self, op_indices: list, dataset_api=None) -> None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  OPTIMIZER_SCOPE = ['stage_1', 'stage_2', 'stage_3']\n",
      " |\n",
      " |  QUERYABLE = True\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from naslib.search_spaces.core.graph.Graph:\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __hash__(self)\n",
      " |      As it is very complicated to compare graphs (i.e. check all edge\n",
      " |      attributes, do the have shared attributes, ...) use just the name\n",
      " |      for comparison.\n",
      " |\n",
      " |      This is used when determining whether two instances are copies.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  add_edges_densly(self)\n",
      " |      Adds edges to get a fully connected DAG without cycles\n",
      " |\n",
      " |  add_node(self, node_index, **attr)\n",
      " |      Adds a node to the graph.\n",
      " |\n",
      " |      Note that adding a node using an index that has been used already\n",
      " |      will override its attributes.\n",
      " |\n",
      " |      Args:\n",
      " |          node_index (int): The index for the node. Expect to be >= 1.\n",
      " |          **attr: The attributes which can be added in a dict like form.\n",
      " |\n",
      " |  clone(self)\n",
      " |      Deep copy of the current graph.\n",
      " |\n",
      " |      Returns:\n",
      " |          Graph: Deep copy of the graph.\n",
      " |\n",
      " |  compile(self)\n",
      " |      Instanciates the ops at the edges using the arguments specified at the edges\n",
      " |\n",
      " |  copy(self)\n",
      " |      Copy as defined in networkx, i.e. a shallow copy.\n",
      " |\n",
      " |      Just handling recursively nested graphs seperately.\n",
      " |\n",
      " |  forward(self, x, *args)\n",
      " |      Forward some data through the graph. This is done recursively\n",
      " |      in case there are graphs defined on nodes or as 'op' on edges.\n",
      " |\n",
      " |      Args:\n",
      " |          x (Tensor or dict): The input. If the graph sits on a node the\n",
      " |              input can be a dict with {source_idx: Tensor} to be routed\n",
      " |              to the defined input nodes. If the graph sits on an edge,\n",
      " |              x is the feature tensor.\n",
      " |          args: This is only required to handle cases where the graph sits\n",
      " |              on an edge and receives an EdgeData object which will be ignored\n",
      " |\n",
      " |  get_all_edge_data(self, key: str, scope='all', private_edge_data: bool = False) -> list\n",
      " |      Get edge attributes of this graph and all child graphs in one go.\n",
      " |\n",
      " |      Args:\n",
      " |          key (str): The key of the attribute\n",
      " |          scope (str): The scope to be applied\n",
      " |          private_edge_data (bool): Whether to return data from graph copies as well.\n",
      " |\n",
      " |      Returns:\n",
      " |          list: All data in a list.\n",
      " |\n",
      " |  get_dense_edges(self)\n",
      " |      Returns the edge indices (i, j) that would make a fully connected\n",
      " |      DAG without circles such that i < j and i != j. Assumes nodes are\n",
      " |      already created.\n",
      " |\n",
      " |      Returns:\n",
      " |          list: list of edge indices.\n",
      " |\n",
      " |  modules_str(self)\n",
      " |      Once the graph has been parsed, prints the modules as they appear in pytorch.\n",
      " |\n",
      " |  num_input_nodes(self) -> int\n",
      " |      The number of input nodes, i.e. the nodes without an\n",
      " |      incoming edge.\n",
      " |\n",
      " |      Returns:\n",
      " |          int: Number of input nodes.\n",
      " |\n",
      " |  parse(self)\n",
      " |      Convert the graph into a neural network which can then\n",
      " |      be optimized by pytorch.\n",
      " |\n",
      " |  prepare_discretization(self)\n",
      " |      In some cases the search space is manipulated before the final\n",
      " |      discretization is happening, e.g. DARTS. In such chases this should\n",
      " |      be defined in the search space, so all optimizers can call it.\n",
      " |\n",
      " |  prepare_evaluation(self)\n",
      " |      In some cases the evaluation architecture does not match the searched\n",
      " |      one. An example is where the makro_model is extended to increase the\n",
      " |      parameters. This is done here.\n",
      " |\n",
      " |  reset_weights(self, inplace: bool = False)\n",
      " |      Resets the weights for the 'op' at all edges.\n",
      " |\n",
      " |      Args:\n",
      " |          inplace (bool): Do the operation in place or\n",
      " |              return a modified copy.\n",
      " |      Returns:\n",
      " |          Graph: Returns the modified version of the graph.\n",
      " |\n",
      " |  set_at_edges(self, key, value, shared=False)\n",
      " |      Sets the attribute for all edges in this and any child graph\n",
      " |\n",
      " |  set_input(self, node_idxs: list)\n",
      " |      Route the input from specific parent edges to the input nodes of\n",
      " |      this subgraph. Inputs are assigned in lexicographical order.\n",
      " |\n",
      " |      Example:\n",
      " |      - Parent node (i.e. node where `self` is located on) has two\n",
      " |        incoming edges from nodes 3 and 5.\n",
      " |      - `self` has two input nodes 1 and 2 (i.e. nodes without\n",
      " |        an incoming edge)\n",
      " |      - `node_idxs = [5, 3]`\n",
      " |      Then input of node 5 is routed to node 1 and input of node 3\n",
      " |      is routed to node 2.\n",
      " |\n",
      " |      Similarly, if `node_idxs = [5, 5]` then input of node 5 is routed\n",
      " |      to both node 1 and 2. Warning: In this case the output of another\n",
      " |      incoming edge is ignored!\n",
      " |\n",
      " |      Should be used in a builder-like pattern: `'subgraph'=Graph().set_input([5, 3])`\n",
      " |\n",
      " |      Args:\n",
      " |          node_idx (list): The index of the nodes where the data is coming from.\n",
      " |\n",
      " |      Returns:\n",
      " |          Graph: self with input node indices set.\n",
      " |\n",
      " |  set_load_labeled(self)\n",
      " |\n",
      " |  set_scope(self, scope: str, recursively=True)\n",
      " |      Sets the scope of this instance of the graph.\n",
      " |\n",
      " |      The function should be used in a builder-like pattern\n",
      " |      `'subgraph'=Graph().set_scope(\"scope\")`.\n",
      " |\n",
      " |      Args:\n",
      " |          scope (str): the scope\n",
      " |          recursively (bool): Also set the scope for all child graphs.\n",
      " |              default True\n",
      " |\n",
      " |      Returns:\n",
      " |          Graph: self with the setted scope.\n",
      " |\n",
      " |  unparse(self)\n",
      " |      Undo the pytorch parsing by reconstructing the graph uusing the\n",
      " |      networkx data structures.\n",
      " |\n",
      " |      This is done recursively also for child graphs.\n",
      " |\n",
      " |      Returns:\n",
      " |          Graph: An unparsed shallow copy of the graph.\n",
      " |\n",
      " |  update_edges(self, update_func: <built-in function callable>, scope='all', private_edge_data: bool = False)\n",
      " |      This updates the edge data of this graph and all child graphs.\n",
      " |      This is the preferred way to manipulate the edges after the definition\n",
      " |      of the graph, e.g. by optimizers who want to insert their own op.\n",
      " |      `update_func(current_edge_data)`. This way optimizers\n",
      " |      can initialize and store necessary information at edges.\n",
      " |\n",
      " |      Note that edges marked as 'final' will not be updated here.\n",
      " |\n",
      " |      Args:\n",
      " |          update_func (callable): Function which accepts one argument called `current_edge_data`. # TODO: Update this. No graphs use this function signature\n",
      " |              and returns the modified EdgeData object.\n",
      " |          scope (str or list(str)): Can be \"all\" or list of scopes to be updated.\n",
      " |          private_edge_data (bool): If set to true, this means update_func will be\n",
      " |              applied to all edges. THIS IS NOT RECOMMENDED FOR SHARED\n",
      " |              ATTRIBUTES. Shared attributes should be set only once, we\n",
      " |              take care it is syncronized across all copies of this graph.\n",
      " |\n",
      " |              The only usecase for setting it to true is when actually changing\n",
      " |              `op` during the initialization of the optimizer (e.g. replacing it\n",
      " |              with MixedOp or SampleOp)\n",
      " |\n",
      " |  update_nodes(self, update_func: <built-in function callable>, scope='all', single_instances: bool = True)\n",
      " |      Update the nodes of the graph and its incoming and outgoing edges by iterating over the\n",
      " |      graph and applying `update_func` to each of it. This is the\n",
      " |      preferred way to change the search space once it has been defined.\n",
      " |\n",
      " |      Note that edges marked as 'final' will not be updated here.\n",
      " |\n",
      " |      Args:\n",
      " |          update_func (callable): Function that accepts three incoming parameters named\n",
      " |              `node, in_edges, out_edges`.\n",
      " |                  - `node` is a tuple (int, dict) containing the\n",
      " |                    index and the attributes of the current node.\n",
      " |                  - `in_edges` is a list of tuples with the index of\n",
      " |                    the tail of the edge and its EdgeData.\n",
      " |                  - `out_edges is a list of tuples with the index of\n",
      " |                    the head of the edge and its EdgeData.\n",
      " |          scope (str or list(str)): Can be \"all\" or list of scopes to be updated. Only graphs\n",
      " |              and child graphs with the specified scope are considered\n",
      " |          single_instance (bool): If set to false, this means update_func will be\n",
      " |              applied to nodes of all copies of a graphs. THIS IS NOT RECOMMENDED FOR SHARED\n",
      " |              ATTRIBUTES, i.e. when manipulating the shared data of incoming or outgoing edges.\n",
      " |              Shared attributes should be set only once, we take care it is syncronized across\n",
      " |              all copies of this graph.\n",
      " |\n",
      " |              The only usecase for setting it to true is when actually changing\n",
      " |              `op` during the initialization of the optimizer (e.g. replacing it\n",
      " |              with MixedOp or SampleOp)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |\n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |\n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |\n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |\n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |\n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |\n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |\n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |\n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |\n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |\n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |\n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |\n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Set the module in evaluation mode.\n",
      " |\n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |\n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |\n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |\n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |\n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |\n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |\n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |\n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |\n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |\n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |\n",
      " |      .. code-block:: text\n",
      " |\n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |\n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |\n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |\n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |\n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |\n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict` unless\n",
      " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |\n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): When ``False``, the properties of the tensors\n",
      " |              in the current module are preserved while when ``True``, the\n",
      " |              properties of the Tensors in the state dict are preserved. The only\n",
      " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
      " |              for which the value from the module is preserved.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing any keys that are expected\n",
      " |                  by this module but missing from the provided ``state_dict``.\n",
      " |              * **unexpected_keys** is a list of str containing the keys that are not\n",
      " |                  expected by this module but present in the provided ``state_dict``.\n",
      " |\n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |\n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |\n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |\n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |\n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |\n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |\n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |\n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |\n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |\n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |\n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |\n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |\n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |\n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |\n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |\n",
      " |      This is typically passed to an optimizer.\n",
      " |\n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |\n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |\n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |\n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |\n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |\n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |\n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |\n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |\n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |\n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |\n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |\n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |\n",
      " |\n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |\n",
      " |          hook(module, args) -> None or modified input\n",
      " |\n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |\n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |\n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |\n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |\n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |\n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |\n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |\n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |\n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |\n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |\n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post hook to be run after module's ``load_state_dict`` is called.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |\n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |\n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |\n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |\n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |\n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |\n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |\n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |\n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |\n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |\n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |\n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  set_extra_state(self, state: Any) -> None\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |\n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |\n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |\n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |\n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |\n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |\n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |\n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |\n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |\n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |\n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |\n",
      " |      This can be called as\n",
      " |\n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |\n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |\n",
      " |      See below for examples.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |\n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |\n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Set the module in training mode.\n",
      " |\n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |\n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |\n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |\n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  T_destination = ~T_destination\n",
      " |\n",
      " |  call_super_init = False\n",
      " |\n",
      " |  dump_patches = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from networkx.classes.digraph.DiGraph:\n",
      " |\n",
      " |  add_edge(self, u_of_edge, v_of_edge, **attr)\n",
      " |      Add an edge between u and v.\n",
      " |\n",
      " |      The nodes u and v will be automatically added if they are\n",
      " |      not already in the graph.\n",
      " |\n",
      " |      Edge attributes can be specified with keywords or by directly\n",
      " |      accessing the edge's attribute dictionary. See examples below.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      u_of_edge, v_of_edge : nodes\n",
      " |          Nodes can be, for example, strings or numbers.\n",
      " |          Nodes must be hashable (and not None) Python objects.\n",
      " |      attr : keyword arguments, optional\n",
      " |          Edge data (or labels or objects) can be assigned using\n",
      " |          keyword arguments.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_edges_from : add a collection of edges\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Adding an edge that already exists updates the edge data.\n",
      " |\n",
      " |      Many NetworkX algorithms designed for weighted graphs use\n",
      " |      an edge attribute (by default `weight`) to hold a numerical value.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      The following all add the edge e=(1, 2) to graph G:\n",
      " |\n",
      " |      >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> e = (1, 2)\n",
      " |      >>> G.add_edge(1, 2)  # explicit two-node form\n",
      " |      >>> G.add_edge(*e)  # single edge as tuple of two nodes\n",
      " |      >>> G.add_edges_from([(1, 2)])  # add edges from iterable container\n",
      " |\n",
      " |      Associate data to edges using keywords:\n",
      " |\n",
      " |      >>> G.add_edge(1, 2, weight=3)\n",
      " |      >>> G.add_edge(1, 3, weight=7, capacity=15, length=342.7)\n",
      " |\n",
      " |      For non-string attribute keys, use subscript notation.\n",
      " |\n",
      " |      >>> G.add_edge(1, 2)\n",
      " |      >>> G[1][2].update({0: 5})\n",
      " |      >>> G.edges[1, 2].update({0: 5})\n",
      " |\n",
      " |  add_edges_from(self, ebunch_to_add, **attr)\n",
      " |      Add all the edges in ebunch_to_add.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ebunch_to_add : container of edges\n",
      " |          Each edge given in the container will be added to the\n",
      " |          graph. The edges must be given as 2-tuples (u, v) or\n",
      " |          3-tuples (u, v, d) where d is a dictionary containing edge data.\n",
      " |      attr : keyword arguments, optional\n",
      " |          Edge data (or labels or objects) can be assigned using\n",
      " |          keyword arguments.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_edge : add a single edge\n",
      " |      add_weighted_edges_from : convenient way to add weighted edges\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Adding the same edge twice has no effect but any edge data\n",
      " |      will be updated when each duplicate edge is added.\n",
      " |\n",
      " |      Edge attributes specified in an ebunch take precedence over\n",
      " |      attributes specified via keyword arguments.\n",
      " |\n",
      " |      When adding edges from an iterator over the graph you are changing,\n",
      " |      a `RuntimeError` can be raised with message:\n",
      " |      `RuntimeError: dictionary changed size during iteration`. This\n",
      " |      happens when the graph's underlying dictionary is modified during\n",
      " |      iteration. To avoid this error, evaluate the iterator into a separate\n",
      " |      object, e.g. by using `list(iterator_of_edges)`, and pass this\n",
      " |      object to `G.add_edges_from`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.add_edges_from([(0, 1), (1, 2)])  # using a list of edge tuples\n",
      " |      >>> e = zip(range(0, 3), range(1, 4))\n",
      " |      >>> G.add_edges_from(e)  # Add the path graph 0-1-2-3\n",
      " |\n",
      " |      Associate data to edges\n",
      " |\n",
      " |      >>> G.add_edges_from([(1, 2), (2, 3)], weight=3)\n",
      " |      >>> G.add_edges_from([(3, 4), (1, 4)], label=\"WN2898\")\n",
      " |\n",
      " |      Evaluate an iterator over a graph if using it to modify the same graph\n",
      " |\n",
      " |      >>> G = nx.DiGraph([(1, 2), (2, 3), (3, 4)])\n",
      " |      >>> # Grow graph by one new node, adding edges to all existing nodes.\n",
      " |      >>> # wrong way - will raise RuntimeError\n",
      " |      >>> # G.add_edges_from(((5, n) for n in G.nodes))\n",
      " |      >>> # right way - note that there will be no self-edge for node 5\n",
      " |      >>> G.add_edges_from(list((5, n) for n in G.nodes))\n",
      " |\n",
      " |  add_nodes_from(self, nodes_for_adding, **attr)\n",
      " |      Add multiple nodes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nodes_for_adding : iterable container\n",
      " |          A container of nodes (list, dict, set, etc.).\n",
      " |          OR\n",
      " |          A container of (node, attribute dict) tuples.\n",
      " |          Node attributes are updated using the attribute dict.\n",
      " |      attr : keyword arguments, optional (default= no attributes)\n",
      " |          Update attributes for all nodes in nodes.\n",
      " |          Node attributes specified in nodes as a tuple take\n",
      " |          precedence over attributes specified via keyword arguments.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_node\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      When adding nodes from an iterator over the graph you are changing,\n",
      " |      a `RuntimeError` can be raised with message:\n",
      " |      `RuntimeError: dictionary changed size during iteration`. This\n",
      " |      happens when the graph's underlying dictionary is modified during\n",
      " |      iteration. To avoid this error, evaluate the iterator into a separate\n",
      " |      object, e.g. by using `list(iterator_of_nodes)`, and pass this\n",
      " |      object to `G.add_nodes_from`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.add_nodes_from(\"Hello\")\n",
      " |      >>> K3 = nx.Graph([(0, 1), (1, 2), (2, 0)])\n",
      " |      >>> G.add_nodes_from(K3)\n",
      " |      >>> sorted(G.nodes(), key=str)\n",
      " |      [0, 1, 2, 'H', 'e', 'l', 'o']\n",
      " |\n",
      " |      Use keywords to update specific node attributes for every node.\n",
      " |\n",
      " |      >>> G.add_nodes_from([1, 2], size=10)\n",
      " |      >>> G.add_nodes_from([3, 4], weight=0.4)\n",
      " |\n",
      " |      Use (node, attrdict) tuples to update attributes for specific nodes.\n",
      " |\n",
      " |      >>> G.add_nodes_from([(1, dict(size=11)), (2, {\"color\": \"blue\"})])\n",
      " |      >>> G.nodes[1][\"size\"]\n",
      " |      11\n",
      " |      >>> H = nx.Graph()\n",
      " |      >>> H.add_nodes_from(G.nodes(data=True))\n",
      " |      >>> H.nodes[1][\"size\"]\n",
      " |      11\n",
      " |\n",
      " |      Evaluate an iterator over a graph if using it to modify the same graph\n",
      " |\n",
      " |      >>> G = nx.DiGraph([(0, 1), (1, 2), (3, 4)])\n",
      " |      >>> # wrong way - will raise RuntimeError\n",
      " |      >>> # G.add_nodes_from(n + 1 for n in G.nodes)\n",
      " |      >>> # correct way\n",
      " |      >>> G.add_nodes_from(list(n + 1 for n in G.nodes))\n",
      " |\n",
      " |  adj = <functools.cached_property object>\n",
      " |      Graph adjacency object holding the neighbors of each node.\n",
      " |\n",
      " |      This object is a read-only dict-like structure with node keys\n",
      " |      and neighbor-dict values.  The neighbor-dict is keyed by neighbor\n",
      " |      to the edge-data-dict.  So `G.adj[3][2]['color'] = 'blue'` sets\n",
      " |      the color of the edge `(3, 2)` to `\"blue\"`.\n",
      " |\n",
      " |      Iterating over G.adj behaves like a dict. Useful idioms include\n",
      " |      `for nbr, datadict in G.adj[n].items():`.\n",
      " |\n",
      " |      The neighbor information is also provided by subscripting the graph.\n",
      " |      So `for nbr, foovalue in G[node].data('foo', default=1):` works.\n",
      " |\n",
      " |      For directed graphs, `G.adj` holds outgoing (successor) info.\n",
      " |\n",
      " |  clear(self)\n",
      " |      Remove all nodes and edges from the graph.\n",
      " |\n",
      " |      This also removes the name, and all graph, node, and edge attributes.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.clear()\n",
      " |      >>> list(G.nodes)\n",
      " |      []\n",
      " |      >>> list(G.edges)\n",
      " |      []\n",
      " |\n",
      " |  clear_edges(self)\n",
      " |      Remove all edges from the graph without altering nodes.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.clear_edges()\n",
      " |      >>> list(G.nodes)\n",
      " |      [0, 1, 2, 3]\n",
      " |      >>> list(G.edges)\n",
      " |      []\n",
      " |\n",
      " |  degree = <functools.cached_property object>\n",
      " |      A DegreeView for the Graph as G.degree or G.degree().\n",
      " |\n",
      " |      The node degree is the number of edges adjacent to the node.\n",
      " |      The weighted node degree is the sum of the edge weights for\n",
      " |      edges incident to that node.\n",
      " |\n",
      " |      This object provides an iterator for (node, degree) as well as\n",
      " |      lookup for the degree for a single node.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges incident to these nodes.\n",
      " |\n",
      " |      weight : string or None, optional (default=None)\n",
      " |         The name of an edge attribute that holds the numerical value used\n",
      " |         as a weight.  If None, then each edge has weight 1.\n",
      " |         The degree is the sum of the edge weights adjacent to the node.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      DiDegreeView or int\n",
      " |          If multiple nodes are requested (the default), returns a `DiDegreeView`\n",
      " |          mapping nodes to their degree.\n",
      " |          If a single node is requested, returns the degree of the node as an integer.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      in_degree, out_degree\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.DiGraph()  # or MultiDiGraph\n",
      " |      >>> nx.add_path(G, [0, 1, 2, 3])\n",
      " |      >>> G.degree(0)  # node 0 with degree 1\n",
      " |      1\n",
      " |      >>> list(G.degree([0, 1, 2]))\n",
      " |      [(0, 1), (1, 2), (2, 2)]\n",
      " |\n",
      " |  edges = <functools.cached_property object>\n",
      " |      An OutEdgeView of the DiGraph as G.edges or G.edges().\n",
      " |\n",
      " |      edges(self, nbunch=None, data=False, default=None)\n",
      " |\n",
      " |      The OutEdgeView provides set-like operations on the edge-tuples\n",
      " |      as well as edge attribute lookup. When called, it also provides\n",
      " |      an EdgeDataView object which allows control of access to edge\n",
      " |      attributes (but does not provide set-like operations).\n",
      " |      Hence, `G.edges[u, v]['color']` provides the value of the color\n",
      " |      attribute for edge `(u, v)` while\n",
      " |      `for (u, v, c) in G.edges.data('color', default='red'):`\n",
      " |      iterates through all the edges yielding the color attribute\n",
      " |      with default `'red'` if no color attribute exists.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges from these nodes.\n",
      " |      data : string or bool, optional (default=False)\n",
      " |          The edge attribute returned in 3-tuple (u, v, ddict[data]).\n",
      " |          If True, return edge attribute dict in 3-tuple (u, v, ddict).\n",
      " |          If False, return 2-tuple (u, v).\n",
      " |      default : value, optional (default=None)\n",
      " |          Value used for edges that don't have the requested attribute.\n",
      " |          Only relevant if data is not True or False.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      edges : OutEdgeView\n",
      " |          A view of edge attributes, usually it iterates over (u, v)\n",
      " |          or (u, v, d) tuples of edges, but can also be used for\n",
      " |          attribute lookup as `edges[u, v]['foo']`.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      in_edges, out_edges\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Nodes in nbunch that are not in the graph will be (quietly) ignored.\n",
      " |      For directed graphs this returns the out-edges.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.DiGraph()  # or MultiDiGraph, etc\n",
      " |      >>> nx.add_path(G, [0, 1, 2])\n",
      " |      >>> G.add_edge(2, 3, weight=5)\n",
      " |      >>> [e for e in G.edges]\n",
      " |      [(0, 1), (1, 2), (2, 3)]\n",
      " |      >>> G.edges.data()  # default data is {} (empty dict)\n",
      " |      OutEdgeDataView([(0, 1, {}), (1, 2, {}), (2, 3, {'weight': 5})])\n",
      " |      >>> G.edges.data(\"weight\", default=1)\n",
      " |      OutEdgeDataView([(0, 1, 1), (1, 2, 1), (2, 3, 5)])\n",
      " |      >>> G.edges([0, 2])  # only edges originating from these nodes\n",
      " |      OutEdgeDataView([(0, 1), (2, 3)])\n",
      " |      >>> G.edges(0)  # only edges from node 0\n",
      " |      OutEdgeDataView([(0, 1)])\n",
      " |\n",
      " |  has_predecessor(self, u, v)\n",
      " |      Returns True if node u has predecessor v.\n",
      " |\n",
      " |      This is true if graph has the edge u<-v.\n",
      " |\n",
      " |  has_successor(self, u, v)\n",
      " |      Returns True if node u has successor v.\n",
      " |\n",
      " |      This is true if graph has the edge u->v.\n",
      " |\n",
      " |  in_degree = <functools.cached_property object>\n",
      " |      An InDegreeView for (node, in_degree) or in_degree for single node.\n",
      " |\n",
      " |      The node in_degree is the number of edges pointing to the node.\n",
      " |      The weighted node degree is the sum of the edge weights for\n",
      " |      edges incident to that node.\n",
      " |\n",
      " |      This object provides an iteration over (node, in_degree) as well as\n",
      " |      lookup for the degree for a single node.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges incident to these nodes.\n",
      " |\n",
      " |      weight : string or None, optional (default=None)\n",
      " |         The name of an edge attribute that holds the numerical value used\n",
      " |         as a weight.  If None, then each edge has weight 1.\n",
      " |         The degree is the sum of the edge weights adjacent to the node.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      If a single node is requested\n",
      " |      deg : int\n",
      " |          In-degree of the node\n",
      " |\n",
      " |      OR if multiple nodes are requested\n",
      " |      nd_iter : iterator\n",
      " |          The iterator returns two-tuples of (node, in-degree).\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      degree, out_degree\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.DiGraph()\n",
      " |      >>> nx.add_path(G, [0, 1, 2, 3])\n",
      " |      >>> G.in_degree(0)  # node 0 with degree 0\n",
      " |      0\n",
      " |      >>> list(G.in_degree([0, 1, 2]))\n",
      " |      [(0, 0), (1, 1), (2, 1)]\n",
      " |\n",
      " |  in_edges = <functools.cached_property object>\n",
      " |      A view of the in edges of the graph as G.in_edges or G.in_edges().\n",
      " |\n",
      " |      in_edges(self, nbunch=None, data=False, default=None):\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges incident to these nodes.\n",
      " |      data : string or bool, optional (default=False)\n",
      " |          The edge attribute returned in 3-tuple (u, v, ddict[data]).\n",
      " |          If True, return edge attribute dict in 3-tuple (u, v, ddict).\n",
      " |          If False, return 2-tuple (u, v).\n",
      " |      default : value, optional (default=None)\n",
      " |          Value used for edges that don't have the requested attribute.\n",
      " |          Only relevant if data is not True or False.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      in_edges : InEdgeView or InEdgeDataView\n",
      " |          A view of edge attributes, usually it iterates over (u, v)\n",
      " |          or (u, v, d) tuples of edges, but can also be used for\n",
      " |          attribute lookup as `edges[u, v]['foo']`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.DiGraph()\n",
      " |      >>> G.add_edge(1, 2, color=\"blue\")\n",
      " |      >>> G.in_edges()\n",
      " |      InEdgeView([(1, 2)])\n",
      " |      >>> G.in_edges(nbunch=2)\n",
      " |      InEdgeDataView([(1, 2)])\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      edges\n",
      " |\n",
      " |  is_directed(self)\n",
      " |      Returns True if graph is directed, False otherwise.\n",
      " |\n",
      " |  is_multigraph(self)\n",
      " |      Returns True if graph is a multigraph, False otherwise.\n",
      " |\n",
      " |  neighbors = successors(self, n)\n",
      " |\n",
      " |  out_degree = <functools.cached_property object>\n",
      " |      An OutDegreeView for (node, out_degree)\n",
      " |\n",
      " |      The node out_degree is the number of edges pointing out of the node.\n",
      " |      The weighted node degree is the sum of the edge weights for\n",
      " |      edges incident to that node.\n",
      " |\n",
      " |      This object provides an iterator over (node, out_degree) as well as\n",
      " |      lookup for the degree for a single node.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges incident to these nodes.\n",
      " |\n",
      " |      weight : string or None, optional (default=None)\n",
      " |         The name of an edge attribute that holds the numerical value used\n",
      " |         as a weight.  If None, then each edge has weight 1.\n",
      " |         The degree is the sum of the edge weights adjacent to the node.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      If a single node is requested\n",
      " |      deg : int\n",
      " |          Out-degree of the node\n",
      " |\n",
      " |      OR if multiple nodes are requested\n",
      " |      nd_iter : iterator\n",
      " |          The iterator returns two-tuples of (node, out-degree).\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      degree, in_degree\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.DiGraph()\n",
      " |      >>> nx.add_path(G, [0, 1, 2, 3])\n",
      " |      >>> G.out_degree(0)  # node 0 with degree 1\n",
      " |      1\n",
      " |      >>> list(G.out_degree([0, 1, 2]))\n",
      " |      [(0, 1), (1, 1), (2, 1)]\n",
      " |\n",
      " |  out_edges = <functools.cached_property object>\n",
      " |      An OutEdgeView of the DiGraph as G.edges or G.edges().\n",
      " |\n",
      " |      edges(self, nbunch=None, data=False, default=None)\n",
      " |\n",
      " |      The OutEdgeView provides set-like operations on the edge-tuples\n",
      " |      as well as edge attribute lookup. When called, it also provides\n",
      " |      an EdgeDataView object which allows control of access to edge\n",
      " |      attributes (but does not provide set-like operations).\n",
      " |      Hence, `G.edges[u, v]['color']` provides the value of the color\n",
      " |      attribute for edge `(u, v)` while\n",
      " |      `for (u, v, c) in G.edges.data('color', default='red'):`\n",
      " |      iterates through all the edges yielding the color attribute\n",
      " |      with default `'red'` if no color attribute exists.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges from these nodes.\n",
      " |      data : string or bool, optional (default=False)\n",
      " |          The edge attribute returned in 3-tuple (u, v, ddict[data]).\n",
      " |          If True, return edge attribute dict in 3-tuple (u, v, ddict).\n",
      " |          If False, return 2-tuple (u, v).\n",
      " |      default : value, optional (default=None)\n",
      " |          Value used for edges that don't have the requested attribute.\n",
      " |          Only relevant if data is not True or False.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      edges : OutEdgeView\n",
      " |          A view of edge attributes, usually it iterates over (u, v)\n",
      " |          or (u, v, d) tuples of edges, but can also be used for\n",
      " |          attribute lookup as `edges[u, v]['foo']`.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      in_edges, out_edges\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Nodes in nbunch that are not in the graph will be (quietly) ignored.\n",
      " |      For directed graphs this returns the out-edges.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.DiGraph()  # or MultiDiGraph, etc\n",
      " |      >>> nx.add_path(G, [0, 1, 2])\n",
      " |      >>> G.add_edge(2, 3, weight=5)\n",
      " |      >>> [e for e in G.edges]\n",
      " |      [(0, 1), (1, 2), (2, 3)]\n",
      " |      >>> G.edges.data()  # default data is {} (empty dict)\n",
      " |      OutEdgeDataView([(0, 1, {}), (1, 2, {}), (2, 3, {'weight': 5})])\n",
      " |      >>> G.edges.data(\"weight\", default=1)\n",
      " |      OutEdgeDataView([(0, 1, 1), (1, 2, 1), (2, 3, 5)])\n",
      " |      >>> G.edges([0, 2])  # only edges originating from these nodes\n",
      " |      OutEdgeDataView([(0, 1), (2, 3)])\n",
      " |      >>> G.edges(0)  # only edges from node 0\n",
      " |      OutEdgeDataView([(0, 1)])\n",
      " |\n",
      " |  pred = <functools.cached_property object>\n",
      " |      Graph adjacency object holding the predecessors of each node.\n",
      " |\n",
      " |      This object is a read-only dict-like structure with node keys\n",
      " |      and neighbor-dict values.  The neighbor-dict is keyed by neighbor\n",
      " |      to the edge-data-dict.  So `G.pred[2][3]['color'] = 'blue'` sets\n",
      " |      the color of the edge `(3, 2)` to `\"blue\"`.\n",
      " |\n",
      " |      Iterating over G.pred behaves like a dict. Useful idioms include\n",
      " |      `for nbr, datadict in G.pred[n].items():`.  A data-view not provided\n",
      " |      by dicts also exists: `for nbr, foovalue in G.pred[node].data('foo'):`\n",
      " |      A default can be set via a `default` argument to the `data` method.\n",
      " |\n",
      " |  predecessors(self, n)\n",
      " |      Returns an iterator over predecessor nodes of n.\n",
      " |\n",
      " |      A predecessor of n is a node m such that there exists a directed\n",
      " |      edge from m to n.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : node\n",
      " |         A node in the graph\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NetworkXError\n",
      " |         If n is not in the graph.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      successors\n",
      " |\n",
      " |  remove_edge(self, u, v)\n",
      " |      Remove the edge between u and v.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      u, v : nodes\n",
      " |          Remove the edge between nodes u and v.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NetworkXError\n",
      " |          If there is not an edge between u and v.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      remove_edges_from : remove a collection of edges\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.Graph()  # or DiGraph, etc\n",
      " |      >>> nx.add_path(G, [0, 1, 2, 3])\n",
      " |      >>> G.remove_edge(0, 1)\n",
      " |      >>> e = (1, 2)\n",
      " |      >>> G.remove_edge(*e)  # unpacks e from an edge tuple\n",
      " |      >>> e = (2, 3, {\"weight\": 7})  # an edge with attribute data\n",
      " |      >>> G.remove_edge(*e[:2])  # select first part of edge tuple\n",
      " |\n",
      " |  remove_edges_from(self, ebunch)\n",
      " |      Remove all edges specified in ebunch.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ebunch: list or container of edge tuples\n",
      " |          Each edge given in the list or container will be removed\n",
      " |          from the graph. The edges can be:\n",
      " |\n",
      " |              - 2-tuples (u, v) edge between u and v.\n",
      " |              - 3-tuples (u, v, k) where k is ignored.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      remove_edge : remove a single edge\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Will fail silently if an edge in ebunch is not in the graph.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> ebunch = [(1, 2), (2, 3)]\n",
      " |      >>> G.remove_edges_from(ebunch)\n",
      " |\n",
      " |  remove_node(self, n)\n",
      " |      Remove node n.\n",
      " |\n",
      " |      Removes the node n and all adjacent edges.\n",
      " |      Attempting to remove a nonexistent node will raise an exception.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : node\n",
      " |         A node in the graph\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NetworkXError\n",
      " |         If n is not in the graph.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      remove_nodes_from\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> list(G.edges)\n",
      " |      [(0, 1), (1, 2)]\n",
      " |      >>> G.remove_node(1)\n",
      " |      >>> list(G.edges)\n",
      " |      []\n",
      " |\n",
      " |  remove_nodes_from(self, nodes)\n",
      " |      Remove multiple nodes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nodes : iterable container\n",
      " |          A container of nodes (list, dict, set, etc.).  If a node\n",
      " |          in the container is not in the graph it is silently ignored.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      remove_node\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      When removing nodes from an iterator over the graph you are changing,\n",
      " |      a `RuntimeError` will be raised with message:\n",
      " |      `RuntimeError: dictionary changed size during iteration`. This\n",
      " |      happens when the graph's underlying dictionary is modified during\n",
      " |      iteration. To avoid this error, evaluate the iterator into a separate\n",
      " |      object, e.g. by using `list(iterator_of_nodes)`, and pass this\n",
      " |      object to `G.remove_nodes_from`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> e = list(G.nodes)\n",
      " |      >>> e\n",
      " |      [0, 1, 2]\n",
      " |      >>> G.remove_nodes_from(e)\n",
      " |      >>> list(G.nodes)\n",
      " |      []\n",
      " |\n",
      " |      Evaluate an iterator over a graph if using it to modify the same graph\n",
      " |\n",
      " |      >>> G = nx.DiGraph([(0, 1), (1, 2), (3, 4)])\n",
      " |      >>> # this command will fail, as the graph's dict is modified during iteration\n",
      " |      >>> # G.remove_nodes_from(n for n in G.nodes if n < 2)\n",
      " |      >>> # this command will work, since the dictionary underlying graph is not modified\n",
      " |      >>> G.remove_nodes_from(list(n for n in G.nodes if n < 2))\n",
      " |\n",
      " |  reverse(self, copy=True)\n",
      " |      Returns the reverse of the graph.\n",
      " |\n",
      " |      The reverse is a graph with the same nodes and edges\n",
      " |      but with the directions of the edges reversed.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      copy : bool optional (default=True)\n",
      " |          If True, return a new DiGraph holding the reversed edges.\n",
      " |          If False, the reverse graph is created using a view of\n",
      " |          the original graph.\n",
      " |\n",
      " |  succ = <functools.cached_property object>\n",
      " |      Graph adjacency object holding the successors of each node.\n",
      " |\n",
      " |      This object is a read-only dict-like structure with node keys\n",
      " |      and neighbor-dict values.  The neighbor-dict is keyed by neighbor\n",
      " |      to the edge-data-dict.  So `G.succ[3][2]['color'] = 'blue'` sets\n",
      " |      the color of the edge `(3, 2)` to `\"blue\"`.\n",
      " |\n",
      " |      Iterating over G.succ behaves like a dict. Useful idioms include\n",
      " |      `for nbr, datadict in G.succ[n].items():`.  A data-view not provided\n",
      " |      by dicts also exists: `for nbr, foovalue in G.succ[node].data('foo'):`\n",
      " |      and a default can be set via a `default` argument to the `data` method.\n",
      " |\n",
      " |      The neighbor information is also provided by subscripting the graph.\n",
      " |      So `for nbr, foovalue in G[node].data('foo', default=1):` works.\n",
      " |\n",
      " |      For directed graphs, `G.adj` is identical to `G.succ`.\n",
      " |\n",
      " |  successors(self, n)\n",
      " |      Returns an iterator over successor nodes of n.\n",
      " |\n",
      " |      A successor of n is a node m such that there exists a directed\n",
      " |      edge from n to m.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : node\n",
      " |         A node in the graph\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NetworkXError\n",
      " |         If n is not in the graph.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      predecessors\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      neighbors() and successors() are the same.\n",
      " |\n",
      " |  to_undirected(self, reciprocal=False, as_view=False)\n",
      " |      Returns an undirected representation of the digraph.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      reciprocal : bool (optional)\n",
      " |        If True only keep edges that appear in both directions\n",
      " |        in the original digraph.\n",
      " |      as_view : bool (optional, default=False)\n",
      " |        If True return an undirected view of the original directed graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      G : Graph\n",
      " |          An undirected graph with the same name and nodes and\n",
      " |          with edge (u, v, data) if either (u, v, data) or (v, u, data)\n",
      " |          is in the digraph.  If both edges exist in digraph and\n",
      " |          their edge data is different, only one edge is created\n",
      " |          with an arbitrary choice of which edge data to use.\n",
      " |          You must check and correct for this manually if desired.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Graph, copy, add_edge, add_edges_from\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      If edges in both directions (u, v) and (v, u) exist in the\n",
      " |      graph, attributes for the new undirected edge will be a combination of\n",
      " |      the attributes of the directed edges.  The edge data is updated\n",
      " |      in the (arbitrary) order that the edges are encountered.  For\n",
      " |      more customized control of the edge attributes use add_edge().\n",
      " |\n",
      " |      This returns a \"deepcopy\" of the edge, node, and\n",
      " |      graph attributes which attempts to completely copy\n",
      " |      all of the data and references.\n",
      " |\n",
      " |      This is in contrast to the similar G=DiGraph(D) which returns a\n",
      " |      shallow copy of the data.\n",
      " |\n",
      " |      See the Python copy module for more information on shallow\n",
      " |      and deep copies, https://docs.python.org/3/library/copy.html.\n",
      " |\n",
      " |      Warning: If you have subclassed DiGraph to use dict-like objects\n",
      " |      in the data structure, those changes do not transfer to the\n",
      " |      Graph created by this method.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(2)  # or MultiGraph, etc\n",
      " |      >>> H = G.to_directed()\n",
      " |      >>> list(H.edges)\n",
      " |      [(0, 1), (1, 0)]\n",
      " |      >>> G2 = H.to_undirected()\n",
      " |      >>> list(G2.edges)\n",
      " |      [(0, 1)]\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from networkx.classes.graph.Graph:\n",
      " |\n",
      " |  __contains__(self, n)\n",
      " |      Returns True if n is a node, False otherwise. Use: 'n in G'.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> 1 in G\n",
      " |      True\n",
      " |\n",
      " |  __getitem__(self, n)\n",
      " |      Returns a dict of neighbors of node n.  Use: 'G[n]'.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : node\n",
      " |         A node in the graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      adj_dict : dictionary\n",
      " |         The adjacency dictionary for nodes connected to n.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      G[n] is the same as G.adj[n] and similar to G.neighbors(n)\n",
      " |      (which is an iterator over G.adj[n])\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G[0]\n",
      " |      AtlasView({1: {}})\n",
      " |\n",
      " |  __iter__(self)\n",
      " |      Iterate over the nodes. Use: 'for n in G'.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      niter : iterator\n",
      " |          An iterator over all nodes in the graph.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> [n for n in G]\n",
      " |      [0, 1, 2, 3]\n",
      " |      >>> list(G)\n",
      " |      [0, 1, 2, 3]\n",
      " |\n",
      " |  __len__(self)\n",
      " |      Returns the number of nodes in the graph. Use: 'len(G)'.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      nnodes : int\n",
      " |          The number of nodes in the graph.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      number_of_nodes: identical method\n",
      " |      order: identical method\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> len(G)\n",
      " |      4\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Returns a short summary of the graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : string\n",
      " |          Graph information including the graph name (if any), graph type, and the\n",
      " |          number of nodes and edges.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.Graph(name=\"foo\")\n",
      " |      >>> str(G)\n",
      " |      \"Graph named 'foo' with 0 nodes and 0 edges\"\n",
      " |\n",
      " |      >>> G = nx.path_graph(3)\n",
      " |      >>> str(G)\n",
      " |      'Graph with 3 nodes and 2 edges'\n",
      " |\n",
      " |  add_weighted_edges_from(self, ebunch_to_add, weight='weight', **attr)\n",
      " |      Add weighted edges in `ebunch_to_add` with specified weight attr\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ebunch_to_add : container of edges\n",
      " |          Each edge given in the list or container will be added\n",
      " |          to the graph. The edges must be given as 3-tuples (u, v, w)\n",
      " |          where w is a number.\n",
      " |      weight : string, optional (default= 'weight')\n",
      " |          The attribute name for the edge weights to be added.\n",
      " |      attr : keyword arguments, optional (default= no attributes)\n",
      " |          Edge attributes to add/update for all edges.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_edge : add a single edge\n",
      " |      add_edges_from : add multiple edges\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Adding the same edge twice for Graph/DiGraph simply updates\n",
      " |      the edge data. For MultiGraph/MultiDiGraph, duplicate edges\n",
      " |      are stored.\n",
      " |\n",
      " |      When adding edges from an iterator over the graph you are changing,\n",
      " |      a `RuntimeError` can be raised with message:\n",
      " |      `RuntimeError: dictionary changed size during iteration`. This\n",
      " |      happens when the graph's underlying dictionary is modified during\n",
      " |      iteration. To avoid this error, evaluate the iterator into a separate\n",
      " |      object, e.g. by using `list(iterator_of_edges)`, and pass this\n",
      " |      object to `G.add_weighted_edges_from`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.add_weighted_edges_from([(0, 1, 3.0), (1, 2, 7.5)])\n",
      " |\n",
      " |      Evaluate an iterator over edges before passing it\n",
      " |\n",
      " |      >>> G = nx.Graph([(1, 2), (2, 3), (3, 4)])\n",
      " |      >>> weight = 0.1\n",
      " |      >>> # Grow graph by one new node, adding edges to all existing nodes.\n",
      " |      >>> # wrong way - will raise RuntimeError\n",
      " |      >>> # G.add_weighted_edges_from(((5, n, weight) for n in G.nodes))\n",
      " |      >>> # correct way - note that there will be no self-edge for node 5\n",
      " |      >>> G.add_weighted_edges_from(list((5, n, weight) for n in G.nodes))\n",
      " |\n",
      " |  adjacency(self)\n",
      " |      Returns an iterator over (node, adjacency dict) tuples for all nodes.\n",
      " |\n",
      " |      For directed graphs, only outgoing neighbors/adjacencies are included.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      adj_iter : iterator\n",
      " |         An iterator over (node, adjacency dictionary) for all nodes in\n",
      " |         the graph.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> [(n, nbrdict) for n, nbrdict in G.adjacency()]\n",
      " |      [(0, {1: {}}), (1, {0: {}, 2: {}}), (2, {1: {}, 3: {}}), (3, {2: {}})]\n",
      " |\n",
      " |  edge_subgraph(self, edges)\n",
      " |      Returns the subgraph induced by the specified edges.\n",
      " |\n",
      " |      The induced subgraph contains each edge in `edges` and each\n",
      " |      node incident to any one of those edges.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      edges : iterable\n",
      " |          An iterable of edges in this graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      G : Graph\n",
      " |          An edge-induced subgraph of this graph with the same edge\n",
      " |          attributes.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The graph, edge, and node attributes in the returned subgraph\n",
      " |      view are references to the corresponding attributes in the original\n",
      " |      graph. The view is read-only.\n",
      " |\n",
      " |      To create a full graph version of the subgraph with its own copy\n",
      " |      of the edge or node attributes, use::\n",
      " |\n",
      " |          G.edge_subgraph(edges).copy()\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(5)\n",
      " |      >>> H = G.edge_subgraph([(0, 1), (3, 4)])\n",
      " |      >>> list(H.nodes)\n",
      " |      [0, 1, 3, 4]\n",
      " |      >>> list(H.edges)\n",
      " |      [(0, 1), (3, 4)]\n",
      " |\n",
      " |  get_edge_data(self, u, v, default=None)\n",
      " |      Returns the attribute dictionary associated with edge (u, v).\n",
      " |\n",
      " |      This is identical to `G[u][v]` except the default is returned\n",
      " |      instead of an exception if the edge doesn't exist.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      u, v : nodes\n",
      " |      default:  any Python object (default=None)\n",
      " |          Value to return if the edge (u, v) is not found.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      edge_dict : dictionary\n",
      " |          The edge attribute dictionary.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G[0][1]\n",
      " |      {}\n",
      " |\n",
      " |      Warning: Assigning to `G[u][v]` is not permitted.\n",
      " |      But it is safe to assign attributes `G[u][v]['foo']`\n",
      " |\n",
      " |      >>> G[0][1][\"weight\"] = 7\n",
      " |      >>> G[0][1][\"weight\"]\n",
      " |      7\n",
      " |      >>> G[1][0][\"weight\"]\n",
      " |      7\n",
      " |\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.get_edge_data(0, 1)  # default edge data is {}\n",
      " |      {}\n",
      " |      >>> e = (0, 1)\n",
      " |      >>> G.get_edge_data(*e)  # tuple form\n",
      " |      {}\n",
      " |      >>> G.get_edge_data(\"a\", \"b\", default=0)  # edge not in graph, return 0\n",
      " |      0\n",
      " |\n",
      " |  has_edge(self, u, v)\n",
      " |      Returns True if the edge (u, v) is in the graph.\n",
      " |\n",
      " |      This is the same as `v in G[u]` without KeyError exceptions.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      u, v : nodes\n",
      " |          Nodes can be, for example, strings or numbers.\n",
      " |          Nodes must be hashable (and not None) Python objects.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      edge_ind : bool\n",
      " |          True if edge is in the graph, False otherwise.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.has_edge(0, 1)  # using two nodes\n",
      " |      True\n",
      " |      >>> e = (0, 1)\n",
      " |      >>> G.has_edge(*e)  #  e is a 2-tuple (u, v)\n",
      " |      True\n",
      " |      >>> e = (0, 1, {\"weight\": 7})\n",
      " |      >>> G.has_edge(*e[:2])  # e is a 3-tuple (u, v, data_dictionary)\n",
      " |      True\n",
      " |\n",
      " |      The following syntax are equivalent:\n",
      " |\n",
      " |      >>> G.has_edge(0, 1)\n",
      " |      True\n",
      " |      >>> 1 in G[0]  # though this gives KeyError if 0 not in G\n",
      " |      True\n",
      " |\n",
      " |  has_node(self, n)\n",
      " |      Returns True if the graph contains the node n.\n",
      " |\n",
      " |      Identical to `n in G`\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : node\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.has_node(0)\n",
      " |      True\n",
      " |\n",
      " |      It is more readable and simpler to use\n",
      " |\n",
      " |      >>> 0 in G\n",
      " |      True\n",
      " |\n",
      " |  nbunch_iter(self, nbunch=None)\n",
      " |      Returns an iterator over nodes contained in nbunch that are\n",
      " |      also in the graph.\n",
      " |\n",
      " |      The nodes in nbunch are checked for membership in the graph\n",
      " |      and if not are silently ignored.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nbunch : single node, container, or all nodes (default= all nodes)\n",
      " |          The view will only report edges incident to these nodes.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      niter : iterator\n",
      " |          An iterator over nodes in nbunch that are also in the graph.\n",
      " |          If nbunch is None, iterate over all nodes in the graph.\n",
      " |\n",
      " |      Raises\n",
      " |      ------\n",
      " |      NetworkXError\n",
      " |          If nbunch is not a node or sequence of nodes.\n",
      " |          If a node in nbunch is not hashable.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      Graph.__iter__\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      When nbunch is an iterator, the returned iterator yields values\n",
      " |      directly from nbunch, becoming exhausted when nbunch is exhausted.\n",
      " |\n",
      " |      To test whether nbunch is a single node, one can use\n",
      " |      \"if nbunch in self:\", even after processing with this routine.\n",
      " |\n",
      " |      If nbunch is not a node or a (possibly empty) sequence/iterator\n",
      " |      or None, a :exc:`NetworkXError` is raised.  Also, if any object in\n",
      " |      nbunch is not hashable, a :exc:`NetworkXError` is raised.\n",
      " |\n",
      " |  nodes = <functools.cached_property object>\n",
      " |      A NodeView of the Graph as G.nodes or G.nodes().\n",
      " |\n",
      " |      Can be used as `G.nodes` for data lookup and for set-like operations.\n",
      " |      Can also be used as `G.nodes(data='color', default=None)` to return a\n",
      " |      NodeDataView which reports specific node data but no set operations.\n",
      " |      It presents a dict-like interface as well with `G.nodes.items()`\n",
      " |      iterating over `(node, nodedata)` 2-tuples and `G.nodes[3]['foo']`\n",
      " |      providing the value of the `foo` attribute for node `3`. In addition,\n",
      " |      a view `G.nodes.data('foo')` provides a dict-like interface to the\n",
      " |      `foo` attribute of each node. `G.nodes.data('foo', default=1)`\n",
      " |      provides a default for nodes that do not have attribute `foo`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : string or bool, optional (default=False)\n",
      " |          The node attribute returned in 2-tuple (n, ddict[data]).\n",
      " |          If True, return entire node attribute dict as (n, ddict).\n",
      " |          If False, return just the nodes n.\n",
      " |\n",
      " |      default : value, optional (default=None)\n",
      " |          Value used for nodes that don't have the requested attribute.\n",
      " |          Only relevant if data is not True or False.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      NodeView\n",
      " |          Allows set-like operations over the nodes as well as node\n",
      " |          attribute dict lookup and calling to get a NodeDataView.\n",
      " |          A NodeDataView iterates over `(n, data)` and has no set operations.\n",
      " |          A NodeView iterates over `n` and includes set operations.\n",
      " |\n",
      " |          When called, if data is False, an iterator over nodes.\n",
      " |          Otherwise an iterator of 2-tuples (node, attribute value)\n",
      " |          where the attribute is specified in `data`.\n",
      " |          If data is True then the attribute becomes the\n",
      " |          entire data dictionary.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      If your node data is not needed, it is simpler and equivalent\n",
      " |      to use the expression ``for n in G``, or ``list(G)``.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      There are two simple ways of getting a list of all nodes in the graph:\n",
      " |\n",
      " |      >>> G = nx.path_graph(3)\n",
      " |      >>> list(G.nodes)\n",
      " |      [0, 1, 2]\n",
      " |      >>> list(G)\n",
      " |      [0, 1, 2]\n",
      " |\n",
      " |      To get the node data along with the nodes:\n",
      " |\n",
      " |      >>> G.add_node(1, time=\"5pm\")\n",
      " |      >>> G.nodes[0][\"foo\"] = \"bar\"\n",
      " |      >>> list(G.nodes(data=True))\n",
      " |      [(0, {'foo': 'bar'}), (1, {'time': '5pm'}), (2, {})]\n",
      " |      >>> list(G.nodes.data())\n",
      " |      [(0, {'foo': 'bar'}), (1, {'time': '5pm'}), (2, {})]\n",
      " |\n",
      " |      >>> list(G.nodes(data=\"foo\"))\n",
      " |      [(0, 'bar'), (1, None), (2, None)]\n",
      " |      >>> list(G.nodes.data(\"foo\"))\n",
      " |      [(0, 'bar'), (1, None), (2, None)]\n",
      " |\n",
      " |      >>> list(G.nodes(data=\"time\"))\n",
      " |      [(0, None), (1, '5pm'), (2, None)]\n",
      " |      >>> list(G.nodes.data(\"time\"))\n",
      " |      [(0, None), (1, '5pm'), (2, None)]\n",
      " |\n",
      " |      >>> list(G.nodes(data=\"time\", default=\"Not Available\"))\n",
      " |      [(0, 'Not Available'), (1, '5pm'), (2, 'Not Available')]\n",
      " |      >>> list(G.nodes.data(\"time\", default=\"Not Available\"))\n",
      " |      [(0, 'Not Available'), (1, '5pm'), (2, 'Not Available')]\n",
      " |\n",
      " |      If some of your nodes have an attribute and the rest are assumed\n",
      " |      to have a default attribute value you can create a dictionary\n",
      " |      from node/attribute pairs using the `default` keyword argument\n",
      " |      to guarantee the value is never None::\n",
      " |\n",
      " |          >>> G = nx.Graph()\n",
      " |          >>> G.add_node(0)\n",
      " |          >>> G.add_node(1, weight=2)\n",
      " |          >>> G.add_node(2, weight=3)\n",
      " |          >>> dict(G.nodes(data=\"weight\", default=1))\n",
      " |          {0: 1, 1: 2, 2: 3}\n",
      " |\n",
      " |  number_of_edges(self, u=None, v=None)\n",
      " |      Returns the number of edges between two nodes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      u, v : nodes, optional (default=all edges)\n",
      " |          If u and v are specified, return the number of edges between\n",
      " |          u and v. Otherwise return the total number of all edges.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      nedges : int\n",
      " |          The number of edges in the graph.  If nodes `u` and `v` are\n",
      " |          specified return the number of edges between those nodes. If\n",
      " |          the graph is directed, this only returns the number of edges\n",
      " |          from `u` to `v`.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      size\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      For undirected graphs, this method counts the total number of\n",
      " |      edges in the graph:\n",
      " |\n",
      " |      >>> G = nx.path_graph(4)\n",
      " |      >>> G.number_of_edges()\n",
      " |      3\n",
      " |\n",
      " |      If you specify two nodes, this counts the total number of edges\n",
      " |      joining the two nodes:\n",
      " |\n",
      " |      >>> G.number_of_edges(0, 1)\n",
      " |      1\n",
      " |\n",
      " |      For directed graphs, this method can count the total number of\n",
      " |      directed edges from `u` to `v`:\n",
      " |\n",
      " |      >>> G = nx.DiGraph()\n",
      " |      >>> G.add_edge(0, 1)\n",
      " |      >>> G.add_edge(1, 0)\n",
      " |      >>> G.number_of_edges(0, 1)\n",
      " |      1\n",
      " |\n",
      " |  number_of_nodes(self)\n",
      " |      Returns the number of nodes in the graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      nnodes : int\n",
      " |          The number of nodes in the graph.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      order: identical method\n",
      " |      __len__: identical method\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.number_of_nodes()\n",
      " |      3\n",
      " |\n",
      " |  order(self)\n",
      " |      Returns the number of nodes in the graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      nnodes : int\n",
      " |          The number of nodes in the graph.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      number_of_nodes: identical method\n",
      " |      __len__: identical method\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(3)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.order()\n",
      " |      3\n",
      " |\n",
      " |  size(self, weight=None)\n",
      " |      Returns the number of edges or total of all edge weights.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weight : string or None, optional (default=None)\n",
      " |          The edge attribute that holds the numerical value used\n",
      " |          as a weight. If None, then each edge has weight 1.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      size : numeric\n",
      " |          The number of edges or\n",
      " |          (if weight keyword is provided) the total weight sum.\n",
      " |\n",
      " |          If weight is None, returns an int. Otherwise a float\n",
      " |          (or more general numeric if the weights are more general).\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      number_of_edges\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.size()\n",
      " |      3\n",
      " |\n",
      " |      >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> G.add_edge(\"a\", \"b\", weight=2)\n",
      " |      >>> G.add_edge(\"b\", \"c\", weight=4)\n",
      " |      >>> G.size()\n",
      " |      2\n",
      " |      >>> G.size(weight=\"weight\")\n",
      " |      6.0\n",
      " |\n",
      " |  subgraph(self, nodes)\n",
      " |      Returns a SubGraph view of the subgraph induced on `nodes`.\n",
      " |\n",
      " |      The induced subgraph of the graph contains the nodes in `nodes`\n",
      " |      and the edges between those nodes.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nodes : list, iterable\n",
      " |          A container of nodes which will be iterated through once.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      G : SubGraph View\n",
      " |          A subgraph view of the graph. The graph structure cannot be\n",
      " |          changed but node/edge attributes can and are shared with the\n",
      " |          original graph.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The graph, edge and node attributes are shared with the original graph.\n",
      " |      Changes to the graph structure is ruled out by the view, but changes\n",
      " |      to attributes are reflected in the original graph.\n",
      " |\n",
      " |      To create a subgraph with its own copy of the edge/node attributes use:\n",
      " |      G.subgraph(nodes).copy()\n",
      " |\n",
      " |      For an inplace reduction of a graph to a subgraph you can remove nodes:\n",
      " |      G.remove_nodes_from([n for n in G if n not in set(nodes)])\n",
      " |\n",
      " |      Subgraph views are sometimes NOT what you want. In most cases where\n",
      " |      you want to do more than simply look at the induced edges, it makes\n",
      " |      more sense to just create the subgraph as its own graph with code like:\n",
      " |\n",
      " |      ::\n",
      " |\n",
      " |          # Create a subgraph SG based on a (possibly multigraph) G\n",
      " |          SG = G.__class__()\n",
      " |          SG.add_nodes_from((n, G.nodes[n]) for n in largest_wcc)\n",
      " |          if SG.is_multigraph():\n",
      " |              SG.add_edges_from(\n",
      " |                  (n, nbr, key, d)\n",
      " |                  for n, nbrs in G.adj.items()\n",
      " |                  if n in largest_wcc\n",
      " |                  for nbr, keydict in nbrs.items()\n",
      " |                  if nbr in largest_wcc\n",
      " |                  for key, d in keydict.items()\n",
      " |              )\n",
      " |          else:\n",
      " |              SG.add_edges_from(\n",
      " |                  (n, nbr, d)\n",
      " |                  for n, nbrs in G.adj.items()\n",
      " |                  if n in largest_wcc\n",
      " |                  for nbr, d in nbrs.items()\n",
      " |                  if nbr in largest_wcc\n",
      " |              )\n",
      " |          SG.graph.update(G.graph)\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(4)  # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
      " |      >>> H = G.subgraph([0, 1, 2])\n",
      " |      >>> list(H.edges)\n",
      " |      [(0, 1), (1, 2)]\n",
      " |\n",
      " |  to_directed(self, as_view=False)\n",
      " |      Returns a directed representation of the graph.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      G : DiGraph\n",
      " |          A directed graph with the same name, same nodes, and with\n",
      " |          each edge (u, v, data) replaced by two directed edges\n",
      " |          (u, v, data) and (v, u, data).\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This returns a \"deepcopy\" of the edge, node, and\n",
      " |      graph attributes which attempts to completely copy\n",
      " |      all of the data and references.\n",
      " |\n",
      " |      This is in contrast to the similar D=DiGraph(G) which returns a\n",
      " |      shallow copy of the data.\n",
      " |\n",
      " |      See the Python copy module for more information on shallow\n",
      " |      and deep copies, https://docs.python.org/3/library/copy.html.\n",
      " |\n",
      " |      Warning: If you have subclassed Graph to use dict-like objects\n",
      " |      in the data structure, those changes do not transfer to the\n",
      " |      DiGraph created by this method.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.Graph()  # or MultiGraph, etc\n",
      " |      >>> G.add_edge(0, 1)\n",
      " |      >>> H = G.to_directed()\n",
      " |      >>> list(H.edges)\n",
      " |      [(0, 1), (1, 0)]\n",
      " |\n",
      " |      If already directed, return a (deep) copy\n",
      " |\n",
      " |      >>> G = nx.DiGraph()  # or MultiDiGraph, etc\n",
      " |      >>> G.add_edge(0, 1)\n",
      " |      >>> H = G.to_directed()\n",
      " |      >>> list(H.edges)\n",
      " |      [(0, 1)]\n",
      " |\n",
      " |  to_directed_class(self)\n",
      " |      Returns the class to use for empty directed copies.\n",
      " |\n",
      " |      If you subclass the base classes, use this to designate\n",
      " |      what directed class to use for `to_directed()` copies.\n",
      " |\n",
      " |  to_undirected_class(self)\n",
      " |      Returns the class to use for empty undirected copies.\n",
      " |\n",
      " |      If you subclass the base classes, use this to designate\n",
      " |      what directed class to use for `to_directed()` copies.\n",
      " |\n",
      " |  update(self, edges=None, nodes=None)\n",
      " |      Update the graph using nodes/edges/graphs as input.\n",
      " |\n",
      " |      Like dict.update, this method takes a graph as input, adding the\n",
      " |      graph's nodes and edges to this graph. It can also take two inputs:\n",
      " |      edges and nodes. Finally it can take either edges or nodes.\n",
      " |      To specify only nodes the keyword `nodes` must be used.\n",
      " |\n",
      " |      The collections of edges and nodes are treated similarly to\n",
      " |      the add_edges_from/add_nodes_from methods. When iterated, they\n",
      " |      should yield 2-tuples (u, v) or 3-tuples (u, v, datadict).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      edges : Graph object, collection of edges, or None\n",
      " |          The first parameter can be a graph or some edges. If it has\n",
      " |          attributes `nodes` and `edges`, then it is taken to be a\n",
      " |          Graph-like object and those attributes are used as collections\n",
      " |          of nodes and edges to be added to the graph.\n",
      " |          If the first parameter does not have those attributes, it is\n",
      " |          treated as a collection of edges and added to the graph.\n",
      " |          If the first argument is None, no edges are added.\n",
      " |      nodes : collection of nodes, or None\n",
      " |          The second parameter is treated as a collection of nodes\n",
      " |          to be added to the graph unless it is None.\n",
      " |          If `edges is None` and `nodes is None` an exception is raised.\n",
      " |          If the first parameter is a Graph, then `nodes` is ignored.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> G = nx.path_graph(5)\n",
      " |      >>> G.update(nx.complete_graph(range(4, 10)))\n",
      " |      >>> from itertools import combinations\n",
      " |      >>> edges = (\n",
      " |      ...     (u, v, {\"power\": u * v})\n",
      " |      ...     for u, v in combinations(range(10, 20), 2)\n",
      " |      ...     if u * v < 225\n",
      " |      ... )\n",
      " |      >>> nodes = [1000]  # for singleton, use a container\n",
      " |      >>> G.update(edges, nodes)\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      It you want to update the graph using an adjacency structure\n",
      " |      it is straightforward to obtain the edges/nodes from adjacency.\n",
      " |      The following examples provide common cases, your adjacency may\n",
      " |      be slightly different and require tweaks of these examples::\n",
      " |\n",
      " |      >>> # dict-of-set/list/tuple\n",
      " |      >>> adj = {1: {2, 3}, 2: {1, 3}, 3: {1, 2}}\n",
      " |      >>> e = [(u, v) for u, nbrs in adj.items() for v in nbrs]\n",
      " |      >>> G.update(edges=e, nodes=adj)\n",
      " |\n",
      " |      >>> DG = nx.DiGraph()\n",
      " |      >>> # dict-of-dict-of-attribute\n",
      " |      >>> adj = {1: {2: 1.3, 3: 0.7}, 2: {1: 1.4}, 3: {1: 0.7}}\n",
      " |      >>> e = [(u, v, {\"weight\": d}) for u, nbrs in adj.items() for v, d in nbrs.items()]\n",
      " |      >>> DG.update(edges=e, nodes=adj)\n",
      " |\n",
      " |      >>> # dict-of-dict-of-dict\n",
      " |      >>> adj = {1: {2: {\"weight\": 1.3}, 3: {\"color\": 0.7, \"weight\": 1.2}}}\n",
      " |      >>> e = [(u, v, {\"weight\": d}) for u, nbrs in adj.items() for v, d in nbrs.items()]\n",
      " |      >>> DG.update(edges=e, nodes=adj)\n",
      " |\n",
      " |      >>> # predecessor adjacency (dict-of-set)\n",
      " |      >>> pred = {1: {2, 3}, 2: {3}, 3: {3}}\n",
      " |      >>> e = [(v, u) for u, nbrs in pred.items() for v in nbrs]\n",
      " |\n",
      " |      >>> # MultiGraph dict-of-dict-of-dict-of-attribute\n",
      " |      >>> MDG = nx.MultiDiGraph()\n",
      " |      >>> adj = {\n",
      " |      ...     1: {2: {0: {\"weight\": 1.3}, 1: {\"weight\": 1.2}}},\n",
      " |      ...     3: {2: {0: {\"weight\": 0.7}}},\n",
      " |      ... }\n",
      " |      >>> e = [\n",
      " |      ...     (u, v, ekey, d)\n",
      " |      ...     for u, nbrs in adj.items()\n",
      " |      ...     for v, keydict in nbrs.items()\n",
      " |      ...     for ekey, d in keydict.items()\n",
      " |      ... ]\n",
      " |      >>> MDG.update(edges=e)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_edges_from: add multiple edges to a graph\n",
      " |      add_nodes_from: add multiple nodes to a graph\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from networkx.classes.graph.Graph:\n",
      " |\n",
      " |  name\n",
      " |      String identifier of the graph.\n",
      " |\n",
      " |      This graph attribute appears in the attribute dict G.graph\n",
      " |      keyed by the string `\"name\"`. as well as an attribute (technically\n",
      " |      a property) `G.name`. This is entirely user controlled.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from networkx.classes.graph.Graph:\n",
      " |\n",
      " |  adjlist_inner_dict_factory = <class 'dict'>\n",
      " |      dict() -> new empty dictionary\n",
      " |      dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |          (key, value) pairs\n",
      " |      dict(iterable) -> new dictionary initialized as if via:\n",
      " |          d = {}\n",
      " |          for k, v in iterable:\n",
      " |              d[k] = v\n",
      " |      dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |          in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |\n",
      " |\n",
      " |  adjlist_outer_dict_factory = <class 'dict'>\n",
      " |      dict() -> new empty dictionary\n",
      " |      dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |          (key, value) pairs\n",
      " |      dict(iterable) -> new dictionary initialized as if via:\n",
      " |          d = {}\n",
      " |          for k, v in iterable:\n",
      " |              d[k] = v\n",
      " |      dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |          in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |\n",
      " |\n",
      " |  edge_attr_dict_factory = <class 'dict'>\n",
      " |      dict() -> new empty dictionary\n",
      " |      dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |          (key, value) pairs\n",
      " |      dict(iterable) -> new dictionary initialized as if via:\n",
      " |          d = {}\n",
      " |          for k, v in iterable:\n",
      " |              d[k] = v\n",
      " |      dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |          in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |\n",
      " |\n",
      " |  graph_attr_dict_factory = <class 'dict'>\n",
      " |      dict() -> new empty dictionary\n",
      " |      dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |          (key, value) pairs\n",
      " |      dict(iterable) -> new dictionary initialized as if via:\n",
      " |          d = {}\n",
      " |          for k, v in iterable:\n",
      " |              d[k] = v\n",
      " |      dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |          in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |\n",
      " |\n",
      " |  node_attr_dict_factory = <class 'dict'>\n",
      " |      dict() -> new empty dictionary\n",
      " |      dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |          (key, value) pairs\n",
      " |      dict(iterable) -> new dictionary initialized as if via:\n",
      " |          d = {}\n",
      " |          for k, v in iterable:\n",
      " |              d[k] = v\n",
      " |      dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |          in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |\n",
      " |\n",
      " |  node_dict_factory = <class 'dict'>\n",
      " |      dict() -> new empty dictionary\n",
      " |      dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |          (key, value) pairs\n",
      " |      dict(iterable) -> new dictionary initialized as if via:\n",
      " |          d = {}\n",
      " |          for k, v in iterable:\n",
      " |              d[k] = v\n",
      " |      dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |          in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture.sample_random_architecture(nasbench201_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cifar10-valid': {'train_losses': [1.9997784596252441,\n",
       "   1.8044536679840089,\n",
       "   1.7075754795074463,\n",
       "   1.6332524340438843,\n",
       "   1.575655322227478,\n",
       "   1.5359442371368408,\n",
       "   1.504173353843689,\n",
       "   1.467329408416748,\n",
       "   1.44943655670166,\n",
       "   1.416760523300171,\n",
       "   1.402323856163025,\n",
       "   1.3875704836654663,\n",
       "   1.3631556325912475,\n",
       "   1.3452378133392333,\n",
       "   1.32651785987854,\n",
       "   1.3105469038772584,\n",
       "   1.300771376953125,\n",
       "   1.2804509231185912,\n",
       "   1.2786731009292602,\n",
       "   1.2698152198791504,\n",
       "   1.2402537381362915,\n",
       "   1.236302218093872,\n",
       "   1.227023156967163,\n",
       "   1.2247301602554321,\n",
       "   1.1942415172576903,\n",
       "   1.1943177279281616,\n",
       "   1.17625702293396,\n",
       "   1.1761633183288573,\n",
       "   1.1708551595687866,\n",
       "   1.1416660580825806,\n",
       "   1.135431934928894,\n",
       "   1.1496392851257324,\n",
       "   1.119645326271057,\n",
       "   1.1080058557891845,\n",
       "   1.1149090364456178,\n",
       "   1.0972071747970582,\n",
       "   1.0900950685119628,\n",
       "   1.094381051750183,\n",
       "   1.0792382767105102,\n",
       "   1.0928624363327026,\n",
       "   1.0622588133239745,\n",
       "   1.0501374659729004,\n",
       "   1.0483271730804444,\n",
       "   1.1004410202789308,\n",
       "   1.040727770690918,\n",
       "   1.0301383501434327,\n",
       "   1.0580180973434448,\n",
       "   1.0270898461914062,\n",
       "   1.0229127894592285,\n",
       "   1.0133065707397462,\n",
       "   1.006662602519989,\n",
       "   0.9987103197479248,\n",
       "   0.9990442742919922,\n",
       "   0.9958386273002624,\n",
       "   0.9786983103752136,\n",
       "   0.9752030908584595,\n",
       "   0.9633893284988403,\n",
       "   0.9697162815475464,\n",
       "   0.9681354219818116,\n",
       "   0.9767951079177857,\n",
       "   0.9489104153060913,\n",
       "   0.9612313977432251,\n",
       "   0.9436843342971801,\n",
       "   0.9374158163452149,\n",
       "   0.9433198051071167,\n",
       "   0.9314581999206543,\n",
       "   0.9291774120903015,\n",
       "   0.9151426500701905,\n",
       "   0.9201643169021606,\n",
       "   0.9148163624572754,\n",
       "   0.9109631428146362,\n",
       "   0.8984385900306702,\n",
       "   0.8877087318229675,\n",
       "   0.8871279279899598,\n",
       "   0.8943322587394714,\n",
       "   0.8851763746833802,\n",
       "   0.8786150309944153,\n",
       "   0.8649573078727723,\n",
       "   0.8701698272705078,\n",
       "   0.8795761253929139,\n",
       "   0.8609301447486878,\n",
       "   0.8534757703781128,\n",
       "   0.8596133228683471,\n",
       "   0.8472026697731018,\n",
       "   0.8467446797180176,\n",
       "   0.8433540546607972,\n",
       "   0.8343961128234864,\n",
       "   0.8278606012535096,\n",
       "   0.833634428691864,\n",
       "   0.8292667283630372,\n",
       "   0.8224720442199707,\n",
       "   0.8132544829750061,\n",
       "   0.8204828254890442,\n",
       "   0.8123710144805908,\n",
       "   0.7975854872322082,\n",
       "   0.8085976768875122,\n",
       "   0.7890243421936035,\n",
       "   0.7879156414413452,\n",
       "   0.7768727870559692,\n",
       "   0.7780814330101014,\n",
       "   0.7763930109786987,\n",
       "   0.7648563170433045,\n",
       "   0.7640554452705384,\n",
       "   0.7652341682624817,\n",
       "   0.7539443430709839,\n",
       "   0.7568673530387878,\n",
       "   0.7521086493682861,\n",
       "   0.7416637401390076,\n",
       "   0.7570195818901062,\n",
       "   0.7408580555152893,\n",
       "   0.7333394758224487,\n",
       "   0.7336051691627502,\n",
       "   0.7179925153541565,\n",
       "   0.7194622675895691,\n",
       "   0.7109456819534302,\n",
       "   0.7106364933395386,\n",
       "   0.7185518634986877,\n",
       "   0.6993908262634277,\n",
       "   0.694808912639618,\n",
       "   0.6845517882156372,\n",
       "   0.685566880645752,\n",
       "   0.6755536685562133,\n",
       "   0.6831321021842957,\n",
       "   0.6738231459045411,\n",
       "   0.6628316062736511,\n",
       "   0.6687152943038941,\n",
       "   0.6578808682632447,\n",
       "   0.6497815762901307,\n",
       "   0.6429996191596985,\n",
       "   0.6510946074676514,\n",
       "   0.6399612430953979,\n",
       "   0.638026661529541,\n",
       "   0.6265298187637329,\n",
       "   0.6227680033493042,\n",
       "   0.6176740060806274,\n",
       "   0.5994999765586853,\n",
       "   0.6073583388710022,\n",
       "   0.606125145740509,\n",
       "   0.597877289352417,\n",
       "   0.5821631155395508,\n",
       "   0.5817775352287292,\n",
       "   0.5806516579818726,\n",
       "   0.5661134750175476,\n",
       "   0.5638368788528443,\n",
       "   0.5621998578643799,\n",
       "   0.5577759298324585,\n",
       "   0.5419700339126587,\n",
       "   0.5442097946166993,\n",
       "   0.537400068063736,\n",
       "   0.5295410024642945,\n",
       "   0.5261390668678284,\n",
       "   0.510366494102478,\n",
       "   0.5094975828742981,\n",
       "   0.49987194650650024,\n",
       "   0.5008065824699401,\n",
       "   0.49505951192855835,\n",
       "   0.4859710004234314,\n",
       "   0.4710111966323853,\n",
       "   0.46722840118408204,\n",
       "   0.4597230913734436,\n",
       "   0.462663008146286,\n",
       "   0.4433710205173492,\n",
       "   0.44518049144744876,\n",
       "   0.44128955713272094,\n",
       "   0.4345816958427429,\n",
       "   0.4224433402824402,\n",
       "   0.41212536794662474,\n",
       "   0.4167300244998932,\n",
       "   0.4002196544551849,\n",
       "   0.3913178194522858,\n",
       "   0.383762574672699,\n",
       "   0.3898267910003662,\n",
       "   0.3676440914726257,\n",
       "   0.35955822870254517,\n",
       "   0.36205807311058047,\n",
       "   0.35865416169166564,\n",
       "   0.3461013995170593,\n",
       "   0.3403681142044067,\n",
       "   0.3336933059024811,\n",
       "   0.32044307456970217,\n",
       "   0.3197073244190216,\n",
       "   0.3205607619667053,\n",
       "   0.30626535373687747,\n",
       "   0.3059521056842804,\n",
       "   0.29992589343070986,\n",
       "   0.2968489152145386,\n",
       "   0.2879332536506653,\n",
       "   0.2855345651912689,\n",
       "   0.2816361360740662,\n",
       "   0.28562423760414124,\n",
       "   0.2765857096290588,\n",
       "   0.27389516543865206,\n",
       "   0.2742867719936371,\n",
       "   0.26508509214878084,\n",
       "   0.2631746364021301,\n",
       "   0.26340217898368834,\n",
       "   0.2639095759487152,\n",
       "   0.2603252219486237,\n",
       "   0.26306646901130676,\n",
       "   0.2640990629005432],\n",
       "  'eval_losses': [2.1027750288391114,\n",
       "   2.345399077835083,\n",
       "   1.7506228700256348,\n",
       "   1.706594264755249,\n",
       "   1.8051812752532959,\n",
       "   1.6717796677780152,\n",
       "   1.6508878505325317,\n",
       "   1.7004136045074463,\n",
       "   1.8309139590072632,\n",
       "   1.584335616874695,\n",
       "   1.6389456813430787,\n",
       "   1.4664493344116212,\n",
       "   1.6419209116363525,\n",
       "   1.5194644136810302,\n",
       "   1.4329804872894287,\n",
       "   1.626332400970459,\n",
       "   1.693599309196472,\n",
       "   1.3813579917526244,\n",
       "   1.765077191429138,\n",
       "   1.374455051803589,\n",
       "   1.5668560597229004,\n",
       "   1.9909547495651245,\n",
       "   1.6632194872665405,\n",
       "   1.503286835975647,\n",
       "   1.617927632751465,\n",
       "   1.786742834739685,\n",
       "   1.8226404067230224,\n",
       "   1.9354247354507446,\n",
       "   1.3576142345809936,\n",
       "   1.4245761727523805,\n",
       "   1.4630439737319947,\n",
       "   1.9435867197418213,\n",
       "   1.3282122219467163,\n",
       "   1.4594522660827636,\n",
       "   1.3552780407333374,\n",
       "   1.488448102684021,\n",
       "   1.206848414077759,\n",
       "   1.556048565635681,\n",
       "   1.3383734684753419,\n",
       "   1.504980724220276,\n",
       "   1.4279162740707398,\n",
       "   1.2400079666900634,\n",
       "   1.4660707851409913,\n",
       "   1.370525365447998,\n",
       "   1.843156643371582,\n",
       "   1.223783782081604,\n",
       "   1.5059891522979736,\n",
       "   1.2856849475097656,\n",
       "   1.2420516245651245,\n",
       "   1.4221962422943115,\n",
       "   1.5149782581710816,\n",
       "   1.651969824256897,\n",
       "   1.832996031112671,\n",
       "   2.060406132736206,\n",
       "   1.5775662384414673,\n",
       "   1.8421261588287354,\n",
       "   1.281154231071472,\n",
       "   1.2672485053634643,\n",
       "   1.3008717073059082,\n",
       "   1.317269567451477,\n",
       "   1.1136664441299438,\n",
       "   1.3318303842544557,\n",
       "   1.5616591723251343,\n",
       "   1.387856301460266,\n",
       "   1.9612160940933228,\n",
       "   1.2197745638656616,\n",
       "   1.228890596847534,\n",
       "   1.3669448958969117,\n",
       "   1.2995572641372681,\n",
       "   1.6883369739151002,\n",
       "   1.2972773790740966,\n",
       "   1.4998590365600586,\n",
       "   1.3892688527679444,\n",
       "   1.2484225469589234,\n",
       "   1.3271551305389404,\n",
       "   2.2300069612121582,\n",
       "   1.2089470999526977,\n",
       "   1.2631909881210328,\n",
       "   1.0612832995224,\n",
       "   1.2955921739959717,\n",
       "   1.5612092876052857,\n",
       "   1.4699037552261351,\n",
       "   1.3903717181777955,\n",
       "   1.3283520230865478,\n",
       "   1.0863673775482179,\n",
       "   1.0728157964897156,\n",
       "   1.2482677797317505,\n",
       "   1.18491514087677,\n",
       "   1.0397233224487306,\n",
       "   1.4734631956100464,\n",
       "   1.6211762363052369,\n",
       "   1.31352568649292,\n",
       "   1.0868615192413331,\n",
       "   1.1426306282043457,\n",
       "   1.0633592637634277,\n",
       "   1.0633407108688355,\n",
       "   0.9736630822944641,\n",
       "   1.175418565673828,\n",
       "   1.1577395094680787,\n",
       "   1.22011266330719,\n",
       "   1.5676763241577147,\n",
       "   1.2006389039611816,\n",
       "   1.1149773036956787,\n",
       "   1.039537625617981,\n",
       "   1.3595482482910157,\n",
       "   1.082355758934021,\n",
       "   0.9504952806663514,\n",
       "   1.0574534102630615,\n",
       "   1.4764803296661377,\n",
       "   1.2031502970504762,\n",
       "   1.1270632553863524,\n",
       "   1.1466573235511779,\n",
       "   1.0399582902526856,\n",
       "   1.365925428123474,\n",
       "   1.7357937954330445,\n",
       "   1.2681050849533082,\n",
       "   1.324150393600464,\n",
       "   1.1678666466522216,\n",
       "   1.134818627166748,\n",
       "   1.468471703224182,\n",
       "   0.9873911561584473,\n",
       "   1.1004687890625,\n",
       "   1.1121667332458496,\n",
       "   1.3394390516662598,\n",
       "   0.9417127799224854,\n",
       "   1.1743708908462525,\n",
       "   1.0157521999359131,\n",
       "   1.0566293659591675,\n",
       "   1.1293798714065553,\n",
       "   0.9916992073822022,\n",
       "   1.079168858642578,\n",
       "   0.9523477384948731,\n",
       "   1.0334128620147705,\n",
       "   1.0859785776519775,\n",
       "   0.904847308921814,\n",
       "   0.8835653739166259,\n",
       "   1.0875660830307008,\n",
       "   0.8878672904205323,\n",
       "   0.9504624750709534,\n",
       "   1.0681207029914856,\n",
       "   0.9647897790145874,\n",
       "   0.9941400630950927,\n",
       "   0.9037885457038879,\n",
       "   0.9965323197937012,\n",
       "   0.8415067425727845,\n",
       "   0.9568633613014221,\n",
       "   0.8834146007537842,\n",
       "   0.8424382433509827,\n",
       "   1.0068197681427002,\n",
       "   0.9147095958518981,\n",
       "   0.851754010066986,\n",
       "   0.9058168306350708,\n",
       "   0.9842578876113892,\n",
       "   0.9898110417366028,\n",
       "   0.8391230449295044,\n",
       "   0.8884210734939575,\n",
       "   0.8544642887687683,\n",
       "   0.8581328844261169,\n",
       "   0.8266747321891784,\n",
       "   0.9093515785598755,\n",
       "   0.881702432346344,\n",
       "   0.8690780821037293,\n",
       "   0.9843150510025025,\n",
       "   0.8428912254142761,\n",
       "   0.8414214466285705,\n",
       "   0.8976057131576538,\n",
       "   0.8361258674240112,\n",
       "   0.8378703596687317,\n",
       "   0.9057886339950562,\n",
       "   0.8677308525085449,\n",
       "   0.8637511229705811,\n",
       "   0.8291324574089051,\n",
       "   0.8449563702392578,\n",
       "   0.8391542455291748,\n",
       "   0.8367960991096497,\n",
       "   0.8640649690628052,\n",
       "   0.8549422912597656,\n",
       "   0.8794158465194702,\n",
       "   0.8535784778785706,\n",
       "   0.864493618850708,\n",
       "   0.8477225358772278,\n",
       "   0.8243977138328552,\n",
       "   0.8319408268356323,\n",
       "   0.8495113105392456,\n",
       "   0.862766034374237,\n",
       "   0.8398259447860718,\n",
       "   0.8473469423675537,\n",
       "   0.8522046714019775,\n",
       "   0.8451504305076599,\n",
       "   0.8519200828361512,\n",
       "   0.8472313698196411,\n",
       "   0.8425611099624634,\n",
       "   0.8407215760421753,\n",
       "   0.8412519073104858,\n",
       "   0.8430823244857788,\n",
       "   0.8442108877754212,\n",
       "   0.8426676008033752,\n",
       "   0.8412381201553345,\n",
       "   0.8401547066116333,\n",
       "   0.8423858296585083,\n",
       "   0.8739288065910339],\n",
       "  'train_acc1es': [23.76399999511719,\n",
       "   31.763999986572266,\n",
       "   36.01999999145508,\n",
       "   39.452,\n",
       "   41.511999985351565,\n",
       "   43.35200000732422,\n",
       "   44.25200000854492,\n",
       "   45.82799999267578,\n",
       "   46.39200000244141,\n",
       "   47.847999998779294,\n",
       "   48.292000004882816,\n",
       "   48.76,\n",
       "   50.10399999023438,\n",
       "   50.532,\n",
       "   51.56400000732422,\n",
       "   52.05199998779297,\n",
       "   52.53599999023437,\n",
       "   53.591999995117185,\n",
       "   53.728000008544925,\n",
       "   54.03199998779297,\n",
       "   55.45199998901367,\n",
       "   55.32399999633789,\n",
       "   55.744,\n",
       "   55.83599999267578,\n",
       "   57.07199998901367,\n",
       "   57.5,\n",
       "   57.67200000244141,\n",
       "   57.49199999633789,\n",
       "   57.859999985351564,\n",
       "   58.83199999389648,\n",
       "   59.55200000732422,\n",
       "   58.62000000488281,\n",
       "   60.00399999511719,\n",
       "   60.415999984130856,\n",
       "   59.93199998901367,\n",
       "   61.387999995117184,\n",
       "   61.18000000366211,\n",
       "   60.99599998535156,\n",
       "   61.85599999145508,\n",
       "   61.271999995117184,\n",
       "   62.17599998413086,\n",
       "   62.84000000244141,\n",
       "   62.695999991455075,\n",
       "   60.71999998657227,\n",
       "   62.932000006103515,\n",
       "   63.304000008544925,\n",
       "   62.49200000244141,\n",
       "   63.71599999511719,\n",
       "   63.88399998779297,\n",
       "   64.04800000732422,\n",
       "   64.57599998535156,\n",
       "   64.72399998657227,\n",
       "   64.71999999267578,\n",
       "   64.80000000732421,\n",
       "   65.36799997070312,\n",
       "   65.69599999023437,\n",
       "   66.04000001953125,\n",
       "   66.09999997558593,\n",
       "   65.9279999975586,\n",
       "   65.5999999975586,\n",
       "   66.45599999267579,\n",
       "   66.27999999267578,\n",
       "   66.75599999023437,\n",
       "   67.18399999755859,\n",
       "   67.02400000488281,\n",
       "   67.18400001708984,\n",
       "   67.46799999267579,\n",
       "   67.69199998291016,\n",
       "   67.58799998779297,\n",
       "   67.80800001464844,\n",
       "   68.01199997802735,\n",
       "   68.23999997070312,\n",
       "   68.79999997070313,\n",
       "   68.87599997802734,\n",
       "   68.70800000244141,\n",
       "   69.208,\n",
       "   69.06,\n",
       "   69.68000001708984,\n",
       "   69.57199997314453,\n",
       "   68.92799997802734,\n",
       "   69.71599997314453,\n",
       "   69.8799999975586,\n",
       "   69.73199997070313,\n",
       "   70.18800001953124,\n",
       "   70.12000001464844,\n",
       "   70.13600001220703,\n",
       "   70.53600001464844,\n",
       "   70.744,\n",
       "   71.0119999951172,\n",
       "   70.92000000488281,\n",
       "   71.02800001464844,\n",
       "   71.176,\n",
       "   71.22400001953125,\n",
       "   71.36000001708985,\n",
       "   72.2400000024414,\n",
       "   71.56400001708984,\n",
       "   71.95600001953125,\n",
       "   72.38400001464844,\n",
       "   72.57200000488281,\n",
       "   72.67600001708985,\n",
       "   72.68799999267578,\n",
       "   73.00000001708985,\n",
       "   73.05199997802734,\n",
       "   72.70799998779297,\n",
       "   73.40800001708985,\n",
       "   73.28000000732422,\n",
       "   73.50800000732421,\n",
       "   73.79599998291016,\n",
       "   73.28799998779297,\n",
       "   74.14800001953125,\n",
       "   74.46799999511718,\n",
       "   73.90800000244141,\n",
       "   74.97999999511718,\n",
       "   74.49599997558593,\n",
       "   74.82399999755859,\n",
       "   74.7959999975586,\n",
       "   74.90800001464844,\n",
       "   75.33999998046875,\n",
       "   75.65999999267578,\n",
       "   75.92399997558594,\n",
       "   75.60400000732422,\n",
       "   76.17599999755859,\n",
       "   76.2280000024414,\n",
       "   76.04800001708985,\n",
       "   76.74399998535156,\n",
       "   76.34800000244141,\n",
       "   76.71599999023438,\n",
       "   77.02399997314453,\n",
       "   77.34,\n",
       "   76.87199999023437,\n",
       "   77.57599998046875,\n",
       "   77.57199999023437,\n",
       "   77.87200001708985,\n",
       "   77.91599999511719,\n",
       "   78.25199999511719,\n",
       "   78.82399999267578,\n",
       "   78.61199998291016,\n",
       "   78.61599999755859,\n",
       "   78.87999997802734,\n",
       "   79.37599998291016,\n",
       "   79.38399997558594,\n",
       "   79.36800001220703,\n",
       "   79.99999999023437,\n",
       "   79.95999997802734,\n",
       "   80.14400000732422,\n",
       "   80.10799998779297,\n",
       "   80.85200001708985,\n",
       "   80.512,\n",
       "   81.04400001708984,\n",
       "   81.09199997802735,\n",
       "   81.37599997558594,\n",
       "   82.15600000976562,\n",
       "   81.93599998046875,\n",
       "   82.55599998291015,\n",
       "   82.38000000976562,\n",
       "   82.31199998046876,\n",
       "   82.82799998291016,\n",
       "   83.22400001220703,\n",
       "   83.52800000488281,\n",
       "   83.74000001708984,\n",
       "   83.4040000024414,\n",
       "   84.18400001464843,\n",
       "   84.22799998535156,\n",
       "   84.38000001464843,\n",
       "   84.65200000488281,\n",
       "   85.09600001708985,\n",
       "   85.31600000976563,\n",
       "   85.47199999267578,\n",
       "   86.02000000732421,\n",
       "   85.94799998779297,\n",
       "   86.65200001464844,\n",
       "   86.428,\n",
       "   87.00799998779297,\n",
       "   87.372,\n",
       "   87.10800001220703,\n",
       "   87.14799999511719,\n",
       "   87.91600000488282,\n",
       "   87.87199999755859,\n",
       "   88.26,\n",
       "   88.75599998291015,\n",
       "   88.89599998779296,\n",
       "   88.83999999023438,\n",
       "   89.30400001708985,\n",
       "   89.23599999023438,\n",
       "   89.69600000488282,\n",
       "   89.74000001708984,\n",
       "   89.92400000732422,\n",
       "   90.07599999023438,\n",
       "   90.308,\n",
       "   90.27999998779296,\n",
       "   90.58399998046875,\n",
       "   90.48799998291015,\n",
       "   90.62799998046874,\n",
       "   90.83199997802734,\n",
       "   90.9919999975586,\n",
       "   91.02399997802735,\n",
       "   90.86399999023438,\n",
       "   91.05599998779297,\n",
       "   90.99599998291016,\n",
       "   91.044],\n",
       "  'eval_acc1es': [23.264000002441406,\n",
       "   24.307999998168945,\n",
       "   34.879999996337894,\n",
       "   36.87999999816895,\n",
       "   35.00799999755859,\n",
       "   39.34799999023438,\n",
       "   40.38000000854492,\n",
       "   38.780000004882815,\n",
       "   37.29199998779297,\n",
       "   41.60399998779297,\n",
       "   41.551999998779294,\n",
       "   45.6479999987793,\n",
       "   44.904000006103516,\n",
       "   44.39599998779297,\n",
       "   48.83999999755859,\n",
       "   42.120000009765626,\n",
       "   45.199999990234375,\n",
       "   50.3319999975586,\n",
       "   40.82399998535156,\n",
       "   50.484,\n",
       "   44.7520000012207,\n",
       "   41.18799999633789,\n",
       "   43.988000004882814,\n",
       "   46.94000000366211,\n",
       "   45.20799999267578,\n",
       "   44.320000001220706,\n",
       "   41.847999997558595,\n",
       "   38.896,\n",
       "   52.25199999145508,\n",
       "   51.672000003662106,\n",
       "   48.643999997558595,\n",
       "   42.33600000976563,\n",
       "   51.94399998901367,\n",
       "   52.71999999267578,\n",
       "   52.40399998779297,\n",
       "   52.171999987792965,\n",
       "   57.77999999633789,\n",
       "   49.88,\n",
       "   53.332000008544924,\n",
       "   49.432000008544925,\n",
       "   52.9520000012207,\n",
       "   56.75200000488281,\n",
       "   51.79199998779297,\n",
       "   53.25999999267578,\n",
       "   44.62400000488281,\n",
       "   57.40800000732422,\n",
       "   48.427999993896485,\n",
       "   56.7640000012207,\n",
       "   56.375999987792966,\n",
       "   51.820000002441404,\n",
       "   53.268000002441404,\n",
       "   48.88799999267578,\n",
       "   46.63200000366211,\n",
       "   45.32000000976562,\n",
       "   51.400000001220704,\n",
       "   47.21200000366211,\n",
       "   57.02800000610352,\n",
       "   57.512000002441404,\n",
       "   54.924,\n",
       "   56.18000000854492,\n",
       "   61.76399999389648,\n",
       "   56.3959999975586,\n",
       "   52.028,\n",
       "   54.359999985351564,\n",
       "   42.57999998779297,\n",
       "   58.28800000732422,\n",
       "   58.691999985351565,\n",
       "   55.67599999633789,\n",
       "   56.56400000854492,\n",
       "   51.648000006103516,\n",
       "   57.49999999633789,\n",
       "   51.16000000488281,\n",
       "   54.14400000854492,\n",
       "   59.33999998901367,\n",
       "   57.59599998657227,\n",
       "   40.9039999987793,\n",
       "   60.90399998291016,\n",
       "   58.45200000610352,\n",
       "   63.52799998779297,\n",
       "   58.36400000488281,\n",
       "   50.46399998779297,\n",
       "   54.783999995117185,\n",
       "   55.804000002441406,\n",
       "   57.90799999023437,\n",
       "   62.77599998413086,\n",
       "   64.55999997802735,\n",
       "   59.1279999975586,\n",
       "   59.667999991455076,\n",
       "   64.39599998046874,\n",
       "   53.99999999267578,\n",
       "   52.520000006103515,\n",
       "   57.13200000488281,\n",
       "   64.28799998046875,\n",
       "   62.70399998413086,\n",
       "   64.66799998413086,\n",
       "   63.83599999023438,\n",
       "   66.66799997558594,\n",
       "   61.931999986572265,\n",
       "   62.15599998901367,\n",
       "   60.895999986572264,\n",
       "   54.60800000610352,\n",
       "   62.599999989013675,\n",
       "   64.29999999023437,\n",
       "   64.8360000048828,\n",
       "   58.22,\n",
       "   64.71999998291015,\n",
       "   68.28400001953125,\n",
       "   64.66399999755859,\n",
       "   53.70000000366211,\n",
       "   62.08400000488281,\n",
       "   63.051999985351564,\n",
       "   63.44399997558594,\n",
       "   65.3799999975586,\n",
       "   55.1640000012207,\n",
       "   50.96799998779297,\n",
       "   63.13599998779297,\n",
       "   60.516000017089844,\n",
       "   63.76400000732422,\n",
       "   63.61600000732422,\n",
       "   55.971999986572264,\n",
       "   67.38400001953126,\n",
       "   65.08400000488281,\n",
       "   64.68400000244141,\n",
       "   58.85599998413086,\n",
       "   68.33199998535156,\n",
       "   63.51999997802734,\n",
       "   67.49199998779297,\n",
       "   65.49599998291016,\n",
       "   65.57999998291015,\n",
       "   68.21600000732421,\n",
       "   66.62399998535156,\n",
       "   69.21599999511719,\n",
       "   67.65999998779297,\n",
       "   67.21199998291016,\n",
       "   69.86799997558593,\n",
       "   71.30000001953125,\n",
       "   66.99199998535157,\n",
       "   71.24799998779297,\n",
       "   69.32400001220704,\n",
       "   67.19200001220703,\n",
       "   69.54800000488281,\n",
       "   68.48399997802734,\n",
       "   71.10400000732422,\n",
       "   69.22799997558593,\n",
       "   72.55200001220703,\n",
       "   69.652,\n",
       "   71.77200000976562,\n",
       "   72.51999997070313,\n",
       "   68.54800001464844,\n",
       "   71.03200000732421,\n",
       "   72.25199998779297,\n",
       "   70.68399999511719,\n",
       "   69.73599998291016,\n",
       "   70.00000001708985,\n",
       "   73.42399997558594,\n",
       "   72.16000001953125,\n",
       "   73.128,\n",
       "   73.28400001464844,\n",
       "   73.87200001220702,\n",
       "   71.92799998046875,\n",
       "   72.64400001953125,\n",
       "   73.52400000244141,\n",
       "   70.26399997070313,\n",
       "   73.87599997802734,\n",
       "   74.33999999267579,\n",
       "   72.58400001708985,\n",
       "   74.54399999267578,\n",
       "   74.28000000976563,\n",
       "   73.02000000488282,\n",
       "   73.69200000732422,\n",
       "   73.73999997070312,\n",
       "   74.99200000732422,\n",
       "   74.64000001464844,\n",
       "   74.97600001464843,\n",
       "   74.54000001953125,\n",
       "   74.22399998291016,\n",
       "   74.51599998535156,\n",
       "   74.33600000976563,\n",
       "   74.97199999023438,\n",
       "   74.99600001220703,\n",
       "   75.52000000976562,\n",
       "   75.79599998535156,\n",
       "   75.48800000732422,\n",
       "   75.50800000732421,\n",
       "   75.17199997558593,\n",
       "   75.73200000244141,\n",
       "   75.668,\n",
       "   75.67200001708984,\n",
       "   75.64399998046875,\n",
       "   75.79599999267577,\n",
       "   75.85999999755859,\n",
       "   75.87199998291015,\n",
       "   76.08800001220703,\n",
       "   75.91200000488281,\n",
       "   75.86399998291016,\n",
       "   75.96799998535157,\n",
       "   75.95599999755859,\n",
       "   75.99600000732421,\n",
       "   76.11199997558593,\n",
       "   75.8799999975586,\n",
       "   75.27],\n",
       "  'cost_info': {'flops': 15.64737,\n",
       "   'params': 0.129306,\n",
       "   'latency': 0.015353918075561523,\n",
       "   'train_time': 7.738443632920583}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture.query(Metric.RAW, \"cifar10\", dataset_api=nasbench201_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbourhood = architecture.get_nbhd(nasbench201_api)\n",
    "len(neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 1, 3, 3, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture.get_op_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "An architecture has already been assigned to this instance of NasBench201SearchSpace. Instantiate a new instance to be able to sample a new model or set a new architecture.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43marchitecture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/Politecnico/Extra/AI-Tech-Lab/NASLib/naslib/search_spaces/nasbench201/graph.py:299\u001b[0m, in \u001b[0;36mNasBench201SearchSpace.mutate\u001b[0;34m(self, parent, dataset_api)\u001b[0m\n\u001b[1;32m    297\u001b[0m op_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(available)\n\u001b[1;32m    298\u001b[0m op_indices[edge] \u001b[38;5;241m=\u001b[39m op_index\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_op_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/Politecnico/Extra/AI-Tech-Lab/NASLib/naslib/search_spaces/nasbench201/graph.py:239\u001b[0m, in \u001b[0;36mNasBench201SearchSpace.set_op_indices\u001b[0;34m(self, op_indices)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_op_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, op_indices: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstantiate_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 239\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m         ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn architecture has already been assigned to this instance of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Instantiate a new instance to be able to sample a new model or set a new architecture.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         convert_op_indices_to_naslib(op_indices, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop_indices \u001b[38;5;241m=\u001b[39m op_indices\n",
      "\u001b[0;31mAssertionError\u001b[0m: An architecture has already been assigned to this instance of NasBench201SearchSpace. Instantiate a new instance to be able to sample a new model or set a new architecture."
     ]
    }
   ],
   "source": [
    "architecture.mutate(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 1, 3, 4, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child = search_spaces.NasBench201SearchSpace()\n",
    "child.mutate(architecture)\n",
    "child.get_op_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAS-Bench-301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/models/surrogates\n",
      "/home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/models/surrogates/nb_models_1.0\n",
      "/home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/models/surrogates/nb_models_1.0/xgb_v1.0\n",
      "/home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/models/surrogates/nb_models_1.0/lgb_runtime_v1.0\n",
      "/home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/models/surrogates/nb301_full_training.pickle\n"
     ]
    }
   ],
   "source": [
    "NASBENCH301_DOWNLOAD_DIR = _REPO_ROOT / \"models\" / \"surrogates\"\n",
    "\n",
    "NASBENCH301_MODEL_DIR = NASBENCH301_DOWNLOAD_DIR / \"nb_models_1.0\"\n",
    "NASBENCH301_PERFORMANCE_DIR = NASBENCH301_MODEL_DIR / \"xgb_v1.0\"\n",
    "NASBENCH301_TIME_DIR = NASBENCH301_MODEL_DIR / \"lgb_runtime_v1.0\"\n",
    "\n",
    "NASBENCH301_BENCHMARK_PATH = NASBENCH301_DOWNLOAD_DIR / \"nb301_full_training.pickle\"\n",
    "\n",
    "print(NASBENCH301_DOWNLOAD_DIR)\n",
    "print(NASBENCH301_MODEL_DIR)\n",
    "print(NASBENCH301_PERFORMANCE_DIR)\n",
    "print(NASBENCH301_TIME_DIR)\n",
    "print(NASBENCH301_BENCHMARK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models 1.0 already at /home/tomaz/git/Politecnico/Extra/AI-Tech-Lab/nas/models/surrogates/nb_models_1.0\n"
     ]
    }
   ],
   "source": [
    "nasbench301.download_models(\n",
    "    version=\"1.0\", delete_zip=True, download_dir=str(NASBENCH301_DOWNLOAD_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_model = nasbench301.load_ensemble(NASBENCH301_PERFORMANCE_DIR)\n",
    "runtime_model = nasbench301.load_ensemble(NASBENCH301_TIME_DIR)\n",
    "\n",
    "nb301_model = [performance_model, runtime_model]\n",
    "with NASBENCH301_BENCHMARK_PATH.open(\"rb\") as f:\n",
    "    nb301_data = pickle.load(f)\n",
    "    nb301_arches = list(nb301_data.keys())\n",
    "\n",
    "nasbench301_api = {\n",
    "    \"nb301_data\": nb301_data,\n",
    "    \"nb301_arches\": nb301_arches,\n",
    "    \"nb301_model\": nb301_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So called \"labelled\" architectures are those from the training set of the surrogate model, that is, those for which training has actually been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph makrograph-0.9668268, scope None, 13 nodes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb301_architecture = search_spaces.NasBench301SearchSpace()\n",
    "nb301_architecture.sample_random_architecture(nasbench301_api, load_labeled=True)\n",
    "nb301_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.01000213623047"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb301_architecture.query(Metric.VAL_ACCURACY, dataset_api=nasbench301_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9230.06887"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb301_architecture.query(Metric.TRAIN_TIME, dataset_api=nasbench301_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb301_neighbourhood = nb301_architecture.get_nbhd(nasbench301_api)\n",
    "len(nb301_neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naslib.search_spaces.nasbench301.graph.NasBench301SearchSpace"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nb301_neighbourhood[0].arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((0, 0), (1, 3), (1, 4), (2, 3), (0, 1), (3, 5), (2, 2), (3, 3)),\n",
       " ((0, 3), (1, 6), (0, 6), (1, 4), (0, 1), (2, 0), (1, 2), (4, 3)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb301_architecture.get_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((0, 0), (1, 3), (1, 4), (2, 3), (0, 1), (3, 5), (2, 2), (3, 3)),\n",
       " ((0, 3), (1, 6), (2, 6), (1, 4), (1, 1), (2, 0), (1, 2), (4, 3)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb301_child = search_spaces.NasBench301SearchSpace()\n",
    "nb301_child.mutate(nb301_architecture)\n",
    "nb301_child.get_compact()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
